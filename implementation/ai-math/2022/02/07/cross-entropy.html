<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Cross Entropy | jinmang2’s Tech blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Cross Entropy" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="PyTorch와 Numpy로 구현하는 Cross Entropy" />
<meta property="og:description" content="PyTorch와 Numpy로 구현하는 Cross Entropy" />
<link rel="canonical" href="https://jinmang2.github.io/implementation/ai-math/2022/02/07/cross-entropy.html" />
<meta property="og:url" content="https://jinmang2.github.io/implementation/ai-math/2022/02/07/cross-entropy.html" />
<meta property="og:site_name" content="jinmang2’s Tech blog" />
<meta property="og:image" content="https://jinmang2.github.io/images/cross_entropy_fig1.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-07T00:00:00-06:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://jinmang2.github.io/images/cross_entropy_fig1.png" />
<meta property="twitter:title" content="Cross Entropy" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-02-07T00:00:00-06:00","datePublished":"2022-02-07T00:00:00-06:00","description":"PyTorch와 Numpy로 구현하는 Cross Entropy","headline":"Cross Entropy","image":"https://jinmang2.github.io/images/cross_entropy_fig1.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://jinmang2.github.io/implementation/ai-math/2022/02/07/cross-entropy.html"},"url":"https://jinmang2.github.io/implementation/ai-math/2022/02/07/cross-entropy.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jinmang2.github.io/feed.xml" title="jinmang2's Tech blog" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-163863775-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-163863775-1');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">jinmang2&#39;s Tech blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Cross Entropy</h1><p class="page-description">PyTorch와 Numpy로 구현하는 Cross Entropy</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-02-07T00:00:00-06:00" itemprop="datePublished">
        Feb 7, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      11 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#Implementation">Implementation</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#AI-math">AI-math</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/jinmang2/jinmang2.github.io/tree/main/_notebooks/2022-02-07-cross-entropy.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/jinmang2/jinmang2.github.io/main?filepath=_notebooks%2F2022-02-07-cross-entropy.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/jinmang2/jinmang2.github.io/blob/main/_notebooks/2022-02-07-cross-entropy.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>

          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fjinmang2%2Fjinmang2.github.io%2Fblob%2Fmain%2F_notebooks%2F2022-02-07-cross-entropy.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#Introduction">Introduction </a></li>
<li class="toc-entry toc-h1"><a href="#Cross-Entropy란?">Cross Entropy란? </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Entropy">Entropy </a></li>
<li class="toc-entry toc-h2"><a href="#이산변수의-Entropy">이산변수의 Entropy </a></li>
<li class="toc-entry toc-h2"><a href="#연속변수의-Entropy">연속변수의 Entropy </a></li>
<li class="toc-entry toc-h2"><a href="#Cross-Entropy">Cross Entropy </a></li>
<li class="toc-entry toc-h2"><a href="#Cross-entropy-and-Maximum-Likelihood-Estimation">Cross-entropy and Maximum Likelihood Estimation </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Code-Implementation">Code Implementation </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Library-호출">Library 호출 </a></li>
<li class="toc-entry toc-h2"><a href="#Numpy로-구현하기:-forward">Numpy로 구현하기: forward </a></li>
<li class="toc-entry toc-h2"><a href="#PyTorch로-구현하기:-forward">PyTorch로 구현하기: forward </a></li>
<li class="toc-entry toc-h2"><a href="#forward-결과값-비교">forward 결과값 비교 </a></li>
<li class="toc-entry toc-h2"><a href="#PyTorch로-구현하기:-backward">PyTorch로 구현하기: backward </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Log-Softmax">Log Softmax </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Negative-Log-Likelihood">Negative Log Likelihood </a></li>
<li class="toc-entry toc-h2"><a href="#Cross-Entropy">Cross Entropy </a></li>
<li class="toc-entry toc-h2"><a href="#put-everything-together">put everything together </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Reference">Reference </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-02-07-cross-entropy.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction<a class="anchor-link" href="#Introduction"> </a>
</h1>
<p>본 포스팅에서는 Cross Entropy에 대해 알아보겠습니다.</p>
<ul>
<li>Cross Entropy의 개념</li>
<li>Cross Entropy 수식</li>
<li>Matplotlib으로 figure 그려보기</li>
<li>Numpy로 코드 구현체 살펴보기</li>
<li>PyTorch로 forward, backward pass 수식과 함께 이해하기</li>
<li>PyTorch 결과값과 동일한 출력이 나오도록 코드 수정</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Cross-Entropy란?">
<a class="anchor" href="#Cross-Entropy%EB%9E%80?" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cross Entropy란?<a class="anchor-link" href="#Cross-Entropy%EB%9E%80?"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Cross entropy는 random variable(확률 변수) 혹은 set of events(사건 집합)이 주어졌을 때 서로 다른 두 확률 분포 사이의 차이를 재는 measure입니다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Entropy">
<a class="anchor" href="#Entropy" aria-hidden="true"><span class="octicon octicon-link"></span></a>Entropy<a class="anchor-link" href="#Entropy"> </a>
</h2>
<p>Event $x$가 등장할 확률을 $P(x)$라고 두고 이에 대한 정보량 $h(x)$에 대한 수식은 아래와 같이 정의됩니다.</p>
<p>
$$h(x)=-\log{P(x)}$$
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$\log$ 함수의 그래프에 음수를 씌우면 아래와 같이 그릴 수 있습니다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"k"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"k"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"probability"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"information"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>위 그래프로부터 아래의 사실을 확인할 수 있습니다.</p>
<ul>
<li>$P(x)$가 <code>0.0</code>에 가까울 때 (사건이 등장할 확률이 희박할 때) 해당 사건에 대한 정보량이 높음을 알 수 있습니다.</li>
<li>$P(x)$가 <code>1.0</code>에 가까울 때 (사건이 등장할 확률이 높을 때) 해당 사건에 대한 정보량이 낮음을 알 수 있습니다.</li>
</ul>
<p>때문에 이를 정보 이론에서는 <code>surprise</code> 라고 묘사합니다. 우리가 매일 마주하는 일상에서는 크게 놀랄만한 정보가 없습니다. 그러나 취업을 했달지 연애를 시작한다던지 인생에 드물게 찾아오는 사건이 생기면 우리는 이 기쁨에 취하며 놀라곤 합니다.</p>
<p>즉, 드물게 발생할 수록 더욱 정보량이 높은 것이지요.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>단일 사건에 대한 정보량말고 set of $x$, $X$를 생각해보겠습니다. $X$는 이산 확률 분포라고 가정할게요. 그러면 다음과 같이 식을 적을 수 있습니다.</p>
<p>
$$H(X)=-\sum_{x\in X}P(x)\cdot\log{P(x)}$$
</p>
<p>$X$의 모든 사건 $x$에 대한 정보량들을 더한 값을 $X$에 대한 정보량으로 생각할 수 있다라고 식이 적혀있습니다. Entropy는 가능한 모든 사건이 발생할 확률이 전부 같을 때 최댓값을 가집니다. 각각의 정보량은 발생할 확률이 작을수록 값이 커지기 때문이죠.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="이산변수의-Entropy">
<a class="anchor" href="#%EC%9D%B4%EC%82%B0%EB%B3%80%EC%88%98%EC%9D%98-Entropy" aria-hidden="true"><span class="octicon octicon-link"></span></a>이산변수의 Entropy<a class="anchor-link" href="#%EC%9D%B4%EC%82%B0%EB%B3%80%EC%88%98%EC%9D%98-Entropy"> </a>
</h2>
<p>Discrete variable의 기댓값은 summation으로 쓸 수 있습니다. 위의 수식을 잘 살펴보죠. 기댓값의 수식이죠?</p>
$$
\begin{aligned}\\
H(X)&amp;=-\sum_{x\in X}P(x)\cdot\log{P(x)}\\
&amp;=\sum_{x\in X}P(x)\cdot(-\log{P(x)})\\
&amp;=\mathbb{E}\big[-\log{P(x)}\big]\\
\end{aligned}
$$<p>위 수식을 잘 기억해주세요. 이산변수는 기댓값을 합으로 계산하기 때문에 아래와 같이 쉽게 계산할 수 있습니다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>확률 분포 Y가 이산분포이고 다음과 같다고 가정해보겠습니다.</p>
<ul>
<li>확률 분포 $Y_1$: $P(Y=0)=0.8,\;P(Y=1)=0.2$</li>
</ul>
<p>$Y_1$에 대한 entropy를 정의된 식과 같이 동일하게 계산할 수 있습니다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">p_y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]</span>

<span class="nb">sum</span><span class="p">([</span><span class="n">p</span> <span class="o">*</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">p_y</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.7219280948873623</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="연속변수의-Entropy">
<a class="anchor" href="#%EC%97%B0%EC%86%8D%EB%B3%80%EC%88%98%EC%9D%98-Entropy" aria-hidden="true"><span class="octicon octicon-link"></span></a>연속변수의 Entropy<a class="anchor-link" href="#%EC%97%B0%EC%86%8D%EB%B3%80%EC%88%98%EC%9D%98-Entropy"> </a>
</h2>
<p>이와는 다르게 Continuous variable의 기댓값은 Integral로 쓸 수 있습니다.</p>
$$
\begin{aligned}\\
H(X)&amp;=\mathbb{E}\big[-\log{P(X)}\big]\\
&amp;=\int_{-\infty}^{\infty}{P(X)\cdot(-\log{P(X)})}dx
\end{aligned}
$$<p>위를 trapezoidal rule을 사용하여 적분값을 계산해보겠습니다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">normal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">mu</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">var</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">var</span><span class="p">))</span>

<span class="n">entropy</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="o">-</span><span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">trapezoidal_rule</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">f</span><span class="p">(</span><span class="n">p</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">f</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span> <span class="o">*</span> <span class="n">dt</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

<span class="n">xlim</span><span class="p">,</span> <span class="n">ylim</span><span class="p">,</span> <span class="n">n_sample</span><span class="p">,</span> <span class="n">n_bin</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">50000</span><span class="p">,</span> <span class="mi">1000</span>
<span class="n">yticks</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="n">mu1</span><span class="p">,</span> <span class="n">mu2</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span>
<span class="n">sigma1</span><span class="p">,</span> <span class="n">sigma2</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.5</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">xlim</span><span class="p">,</span> <span class="n">xlim</span><span class="p">,</span> <span class="n">n_sample</span><span class="p">)</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_sample</span><span class="p">))</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_sample</span><span class="p">))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">plot_normal_entropy</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">n_bin</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s2">"stepfilled"</span><span class="p">,</span> 
                         <span class="n">color</span><span class="o">=</span><span class="s2">"slateblue"</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">"k"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">normal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">"k"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ylim</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">yticks</span><span class="p">)</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">bins</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">normal</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="n">ent</span> <span class="o">=</span> <span class="n">trapezoidal_rule</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">entropy</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">rf</span><span class="s2">"N($</span><span class="si">{</span><span class="n">mu</span><span class="si">}</span><span class="s2">$, $</span><span class="si">{</span><span class="n">sigma</span><span class="si">}</span><span class="s2">^2$)  H(p)=</span><span class="si">{</span><span class="n">ent</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">plot_normal_entropy</span><span class="p">(</span><span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">X1</span><span class="p">,</span> <span class="n">mu1</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"probability"</span><span class="p">)</span>
<span class="n">plot_normal_entropy</span><span class="p">(</span><span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">X2</span><span class="p">,</span> <span class="n">mu2</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>위처럼 직접 적분값을 근사하여 계산할 수도 있지만 정규 분포 엔트로피 값을 해석적으로도 계산할 수 있습니다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>정규 분포의 수식은 아래와 같습니다.</p>
<p>
$$P(X)=\cfrac{1}{\sqrt{2\pi\sigma^2}}\exp{\bigg(\cfrac{-(X-\mu)^2}{2\sigma^2}\bigg)}$$
</p>
<p>이제 엔트로피 수식을 전개해봅시다.</p>
$$
\begin{aligned}\\
H(X)&amp;=\mathbb{E}\big[-\log{P(X)}\big]\\
&amp;=\int_{-\infty}^{\infty}{P(X)\cdot(-\log{P(X)})}dX
\end{aligned}
$$<p>여기서 $-\log P(X)$는 $\cfrac{1}{2}\log{2\pi\sigma^2}+\cfrac{1}{2\sigma^2}(X-\mu)^2$이고 이를 대입하면</p>
$$
H(X)=\cfrac{1}{2}\log{2\pi\sigma^2}\int_{-\infty}^{\infty}{P(X)}dX+\cfrac{1}
{2\sigma^2}\int_{-\infty}^{\infty}(X-\mu)^2P(X)dX
$$<p>여기서 확률 및 분산의 정의에 따라 다음을 알 수 있고</p>
<p>
$$\int_{-\infty}^{\infty}{P(X)}dX=1$$


$$\int_{-\infty}^{\infty}(X-\mu)^2P(X)dX=\sigma^2$$
</p>
<p>이를 사용하여 식을 다시 작성하면</p>
<p>
$$H(X)=\cfrac{1}{2}\big(\log{2\pi\sigma^2}+1\big)$$
</p>
<p>와 같이 쓸 수 있습니다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>python code로 구현하여 위의 결과와 비교해봅시다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">normal_entropy</span><span class="p">(</span><span class="n">sigma</span><span class="p">):</span>
    <span class="c1"># return 0.5 * np.log(np.e * 2 * np.pi * sigma**2)</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="n">normal_entropy</span><span class="p">(</span><span class="n">sigma1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>1.4189385332046727</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Cross-Entropy">
<a class="anchor" href="#Cross-Entropy" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cross Entropy<a class="anchor-link" href="#Cross-Entropy"> </a>
</h2>
<p>Machine Learning: A Probabilistic Perspective에서는 cross entropy를 다음과 같이 설명합니다.</p>

<pre><code>The cross entropy is the average number of bits needed to encode data coming from a source with distribution p when we use model q ...</code></pre>
<p>$P$는 모델링하고자 하는 분포, $Q$는 모델링을 할 분포라고 생각하면 이해가 빠릅니다. $P$ 대신 $Q$를 사용하여 사건을 나타내는 총 비트의 평균 수 입니다.</p>
<p>
$$H(P,Q)=-\sum_{x\in X}P(x)\cdot\log Q(x)$$
</p>
<p>총 비트 수가 아니라 추가로 필요한 비트의 평균값은? <code>relative entropy</code>, <code>Kullback-Leibler Divergence</code> 라고 합니다.</p>
<p>
$$KL(P||Q)=-\sum_{x\in X}P(x)\cdot\log{\cfrac{Q(x)}{P(x)}}$$
</p>
<p>위 수식은 Entropy와 엮어서 다음과 같이 쓸 수 있습니다.</p>
$$
\begin{aligned}\\
H(P)&amp;=\sum_{x \in X} P(x)\cdot\log P(x)\\
&amp;=H(P,Q)-KL(P||Q)
\end{aligned}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Cross-entropy-and-Maximum-Likelihood-Estimation">
<a class="anchor" href="#Cross-entropy-and-Maximum-Likelihood-Estimation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cross-entropy and Maximum Likelihood Estimation<a class="anchor-link" href="#Cross-entropy-and-Maximum-Likelihood-Estimation"> </a>
</h2>
<p>분류 문제를 풀기 위해 Neural Network를 학습시킬 때, 우리는 흔히 Cross Entropy로 학습시킵니다. 왜일까요?</p>
<p>위에서 Entropy, Cross Entropy, KL-Divergence에 대한 수식을 정의했습니다.</p>
<p>앞서 확률 변수의 Entropy 정의에서 Entropy가 확률 변수의 Expectation과 관련이 있음을 확인했었습니다. 각각을 기댓값 표현으로 써보면,</p>
$$
\begin{aligned}\\
H(X)&amp;=-\sum_x p(x)\log p(x)\\
&amp;=\sum_x -p(x)\log p(x)\\
&amp;=\sum_x p(x)\log \cfrac{1}{p(x)}\\
&amp;=\mathbb{E}_{X\sim p(x)}\bigg[\log \cfrac{1}{p(x)}\bigg]
\end{aligned}
$$$$
\begin{aligned}\\
KL(p||q)&amp;=\sum_x p(x)\log \cfrac{p(x)}{q(x)}\\
&amp;=\mathbb{E}_{X\sim p(x)}\bigg[\log\cfrac{p(x)}{q(x)}\bigg]\\
&amp;=\mathbb{E}_{X\sim p(x)}\bigg[\log\cfrac{1}{q(x)}-\cfrac{1}{p(x)}\bigg]\\
&amp;=\mathbb{E}_{X\sim p(x)}\bigg[\log\cfrac{1}{q(x)}\bigg]-\mathbb{E}_{X\sim p(x)}\bigg[\log\cfrac{1}{p(x)}\bigg]\\
&amp;=\mathbb{E}_{X\sim p(x)}\bigg[\log\cfrac{1}{q(x)}\bigg] - H(p)
\end{aligned}
$$<p>Cross entropy는 아래와 같이 적을 수 있죠.</p>
<p>
$$H(p,q)=\mathbb{E}_{X\sim p(x)}\bigg[\log\cfrac{1}{q(x)}\bigg]$$
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>그러면 Maximum Likelihood Estimation에서의 objective function을 살펴보겠습니다.</p>
<p>우리는 분포 $p$를 모델링하고 싶습니다. 이에 대한 데이터 $X$를 모수 $\theta$를 가지는 parametric model $q$로 모델링하면 아래와 같이 수식을 쓸 수 있습니다.</p>
<p>
$$\theta_{ML}=\underset{\theta}{\mathrm{argmax}}\;q(X;\theta)$$
</p>
<p>i.i.d 가정에 의해 아래와 같이 수식을 작성하면 (로그 스케일로)</p>
$$
\begin{aligned}\\
\theta_{ML}&amp;=\underset{\theta}{\mathrm{argmax}}\;\prod_{i=1}^{m}q(x_i;\theta)\\
&amp;=\underset{\theta}{\mathrm{argmax}}\;\sum_{i=1}^{m}\log q(x_i;\theta)\\
&amp;=\underset{\theta}{\mathrm{argmax}}\;\sum_{i=1}^{m}\frac{1}{m}\log q(x_i;\theta)\\
&amp;=\underset{\theta}{\mathrm{argmax}}\;\mathbb{E}_{X\sim p(x)}\big[\log q(x)\big]\\
&amp;=\underset{\theta}{\mathrm{argmin}}\;-\mathbb{E}_{X\sim p(x)}\big[\log q(x)\big]\\
&amp;=\underset{\theta}{\mathrm{argmin}}\;\mathbb{E}_{X\sim p(x)}\big[\log\frac{1}{q(x)}\big]\\
\end{aligned}
$$<p>즉, Likelihood를 maximize하는 문제는 Cross Entropy를 Minimize하는 문제로 치환할 수 있습니다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Code-Implementation">
<a class="anchor" href="#Code-Implementation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Code Implementation<a class="anchor-link" href="#Code-Implementation"> </a>
</h1>
<p>Cross Entropy를 구현해봅시다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Library-호출">
<a class="anchor" href="#Library-%ED%98%B8%EC%B6%9C" aria-hidden="true"><span class="octicon octicon-link"></span></a>Library 호출<a class="anchor-link" href="#Library-%ED%98%B8%EC%B6%9C"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Any</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">_TensorOrTensors</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Numpy로-구현하기:-forward">
<a class="anchor" href="#Numpy%EB%A1%9C-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0:-forward" aria-hidden="true"><span class="octicon octicon-link"></span></a>Numpy로 구현하기: forward<a class="anchor-link" href="#Numpy%EB%A1%9C-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0:-forward"> </a>
</h2>
<p>Cross Entropy는 <code>LogSoftmax</code>와 <code>NegativeLogLikelihood</code>로 계산할 수 있습니다. 위에서 본 것처럼 우도를 최대화하는 문제와 Cross entropy를 최소화하는 문제는 동일하기 때문에 이를 Negative Log likelihood를 최소화하는 문제로 생각해도 무방하겠죠? 이를 활용하여 Cross entropy를 구해봅시다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>한 가지, 구현 테크닉을 소개드리고자 합니다. softmax 함수는 다음과 같은 특징을 가지고 있습니다.</p>
<p>
$$\mathrm{softmax}(x)=\mathrm{softmax}(x+c)$$
</p>
<p>이를 활용하여 overflow 문제를 해결할 수 있습니다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>
$$\mathrm{softmax}(x_i)=\cfrac{exp(x_i)}{\sum_j exp(x_j)}$$
</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">log_softmax_numpy</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">arr</span> <span class="o">-</span> <span class="n">c</span>
    <span class="n">nominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">nominator</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">nominator</span> <span class="o">/</span> <span class="n">denominator</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>우도는 fancy indexing으로 간단하게 계산할 수 있습니다. 구한 likelihood에 음수를 씌워주면 Negative Likelihood가 되겠지요.</p>
<p>구한 Negative Log Likelihood는 평균값으로 reduce하겠습니다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">negative_log_likelihood_numpy</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">y</span><span class="p">]</span>
    <span class="n">nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_likelihood</span>
    <span class="k">return</span> <span class="n">nll</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Cross entropy는 이제 간단합니다. 예측 값 Q에 log softmax를 취해주고 음의 로그 가능도를 계산해주면 됩니다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">cross_entropy_numpy</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_softmax_numpy</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="n">ce_loss</span> <span class="o">=</span> <span class="n">negative_log_likelihood_numpy</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ce_loss</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="PyTorch로-구현하기:-forward">
<a class="anchor" href="#PyTorch%EB%A1%9C-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0:-forward" aria-hidden="true"><span class="octicon octicon-link"></span></a>PyTorch로 구현하기: forward<a class="anchor-link" href="#PyTorch%EB%A1%9C-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0:-forward"> </a>
</h2>
<p>이를 pytorch로도 구현해봅시다. 구현은 메서드명까지 동일합니다. 차이라면 numpy에서는 차원축을 axis라는 parameter로 받는 반면, torch는 dim이라는 parameter로 받습니다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">log_softmax_torch</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">tensor</span> <span class="o">-</span> <span class="n">c</span>
    <span class="n">nominator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">nominator</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">nominator</span> <span class="o">/</span> <span class="n">denominator</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">negative_log_likelihood_torch</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">y</span><span class="p">]</span>
    <span class="n">nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_likelihood</span>
    <span class="k">return</span> <span class="n">nll</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">cross_entropy_torch</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_softmax_torch</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="n">ce_loss</span> <span class="o">=</span> <span class="n">negative_log_likelihood_torch</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ce_loss</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="forward-결과값-비교">
<a class="anchor" href="#forward-%EA%B2%B0%EA%B3%BC%EA%B0%92-%EB%B9%84%EA%B5%90" aria-hidden="true"><span class="octicon octicon-link"></span></a>forward 결과값 비교<a class="anchor-link" href="#forward-%EA%B2%B0%EA%B3%BC%EA%B0%92-%EB%B9%84%EA%B5%90"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">3000</span>

<span class="n">rtol</span> <span class="o">=</span> <span class="mf">1e-4</span>
<span class="n">atol</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="n">isclose</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">normalvariate</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
<span class="p">]</span>
<span class="n">y_pred_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
<span class="n">y_pred_torch</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">y_pred_numpy</span> <span class="o">=</span> <span class="n">y_pred_torch</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]</span>
<span class="n">y_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">y_numpy</span> <span class="o">=</span> <span class="n">y_torch</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ce_result</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()(</span><span class="n">y_pred_torch</span><span class="p">,</span> <span class="n">y_torch</span><span class="p">)</span>
<span class="n">ce_numpy</span> <span class="o">=</span> <span class="n">cross_entropy_numpy</span><span class="p">(</span><span class="n">y_pred_numpy</span><span class="p">,</span> <span class="n">y_numpy</span><span class="p">)</span>
<span class="n">ce_torch</span> <span class="o">=</span> <span class="n">cross_entropy_torch</span><span class="p">(</span><span class="n">y_pred_torch</span><span class="p">,</span> <span class="n">y_torch</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">isclose</span><span class="p">(</span><span class="n">ce_result</span><span class="p">,</span> <span class="n">ce_torch</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">isclose</span><span class="p">(</span><span class="n">ce_result</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ce_numpy</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">success</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span><span class="p">:</span>
    <span class="n">success</span> <span class="o">=</span> <span class="kc">False</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Do both output the same tensors?"</span><span class="p">,</span> <span class="s2">"🔥"</span> <span class="k">if</span> <span class="n">success</span> <span class="k">else</span> <span class="s2">"💩"</span><span class="p">)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">success</span><span class="p">:</span>
    <span class="k">raise</span> <span class="n">Exeption</span><span class="p">(</span><span class="s2">"Something went wrong"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Do both output the same tensors? 🔥
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="PyTorch로-구현하기:-backward">
<a class="anchor" href="#PyTorch%EB%A1%9C-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0:-backward" aria-hidden="true"><span class="octicon octicon-link"></span></a>PyTorch로 구현하기: backward<a class="anchor-link" href="#PyTorch%EB%A1%9C-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0:-backward"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Log-Softmax">
<a class="anchor" href="#Log-Softmax" aria-hidden="true"><span class="octicon octicon-link"></span></a>Log Softmax<a class="anchor-link" href="#Log-Softmax"> </a>
</h3>
<p>
$$o(1-o)$$
</p>
<ul>
<li>1은 크로네클 델타</li>
<li>log softmax가 계산 상 이점이 큼</li>
<li>loss를 더 크게 만들어 주기도</li>
<li>softmax의 backward의 grad_outputs에 log 함수의 역함수인 1/x를 넣어주면 log_softmax의 backward form이 나온다.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">_softmax</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">tensor</span> <span class="o">-</span> <span class="n">c</span>
    <span class="n">nominator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">nominator</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">nominator</span> <span class="o">/</span> <span class="n">denominator</span>
    <span class="k">return</span> <span class="n">probs</span>


<span class="k">class</span> <span class="nc">Softmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">_softmax</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">probs</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="n">probs</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">grad_outputs</span> <span class="o">-=</span> <span class="p">(</span><span class="n">grad_outputs</span> <span class="o">*</span> <span class="n">probs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">probs</span> <span class="o">*</span> <span class="n">grad_outputs</span><span class="p">,</span> <span class="kc">None</span>


<span class="n">softmax</span> <span class="o">=</span> <span class="n">Softmax</span><span class="o">.</span><span class="n">apply</span>


<span class="k">class</span> <span class="nc">LogSoftmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">_softmax</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="n">probs</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">grad_outputs</span> <span class="o">-=</span> <span class="n">probs</span> <span class="o">*</span> <span class="n">grad_outputs</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_outputs</span><span class="p">,</span> <span class="kc">None</span>
    

<span class="n">log_softmax</span> <span class="o">=</span> <span class="n">LogSoftmax</span><span class="o">.</span><span class="n">apply</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Softmax check</span>
<span class="c1"># -- forward pass</span>
<span class="n">y_orig</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_impl</span> <span class="o">=</span> <span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">isclose</span><span class="p">(</span><span class="n">y_orig</span><span class="p">,</span> <span class="n">y_impl</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">"💩"</span>
<span class="c1"># -- backward pass</span>
<span class="n">dy_orig</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">y_orig</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">dy_impl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">y_impl</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">isclose</span><span class="p">(</span><span class="n">dy_orig</span><span class="p">,</span> <span class="n">dy_impl</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">"💩"</span>

<span class="c1"># LogSoftmax check</span>
<span class="c1"># -- forward pass</span>
<span class="n">y_orig</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_impl</span> <span class="o">=</span> <span class="n">y</span> <span class="o">=</span> <span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">isclose</span><span class="p">(</span><span class="n">y_orig</span><span class="p">,</span> <span class="n">y_impl</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">"💩"</span>
<span class="c1"># -- backward pass</span>
<span class="n">dy_orig</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">y_orig</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">dy_impl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">y_impl</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">isclose</span><span class="p">(</span><span class="n">dy_orig</span><span class="p">,</span> <span class="n">dy_impl</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">"💩"</span>

<span class="c1"># Log + Softmax check</span>
<span class="c1"># -- forward pass</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">isclose</span><span class="p">(</span><span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">"💩"</span>
<span class="c1"># -- backward pass</span>
<span class="n">dy1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">y1</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">dy2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">y2</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">isclose</span><span class="p">(</span><span class="n">dy1</span><span class="p">,</span> <span class="n">dy2</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">"💩"</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"🔥"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>🔥
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Negative-Log-Likelihood">
<a class="anchor" href="#Negative-Log-Likelihood" aria-hidden="true"><span class="octicon octicon-link"></span></a>Negative Log Likelihood<a class="anchor-link" href="#Negative-Log-Likelihood"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">NegativeLogLikelihoodLoss</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="n">bsz</span><span class="p">,</span> <span class="n">n_classes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bsz</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span>
        <span class="n">nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_likelihood</span>
        <span class="k">return</span> <span class="n">nll</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="n">bsz</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">grad_outputs</span> <span class="o">=</span> <span class="n">grad_outputs</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">bsz</span><span class="p">)</span> <span class="o">/</span> <span class="n">bsz</span>
        <span class="n">negative_grad</span> <span class="o">=</span> <span class="o">-</span><span class="n">grad_outputs</span>
        <span class="n">ll_grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">grad_outputs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">ll_grad</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bsz</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
        <span class="n">grad_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">negative_grad</span><span class="p">)</span> <span class="o">@</span> <span class="n">ll_grad</span>
        <span class="k">return</span> <span class="n">grad_outputs</span><span class="p">,</span> <span class="kc">None</span>
    
<span class="n">nll_loss</span> <span class="o">=</span> <span class="n">NegativeLogLikelihoodLoss</span><span class="o">.</span><span class="n">apply</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Cross-Entropy">
<a class="anchor" href="#Cross-Entropy" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cross Entropy<a class="anchor-link" href="#Cross-Entropy"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">y_pred</span><span class="p">:</span> <span class="n">_TensorOrTensors</span><span class="p">,</span> 
        <span class="n">y</span><span class="p">:</span> <span class="n">_TensorOrTensors</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_TensorOrTensors</span><span class="p">:</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_softmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">ce_loss</span> <span class="o">=</span> <span class="n">nll_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span> <span class="o">/</span> <span class="n">log_probs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">ce_loss</span>
    
    <span class="k">def</span> <span class="nf">save_for_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">saved_tensors</span> <span class="o">=</span> <span class="n">args</span>
        
    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">:</span> <span class="n">_TensorOrTensors</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_TensorOrTensors</span><span class="p">:</span>
        <span class="n">probs</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">ce_grad</span> <span class="o">=</span> <span class="n">probs</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_outputs</span> <span class="o">*</span> <span class="n">ce_grad</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="put-everything-together">
<a class="anchor" href="#put-everything-together" aria-hidden="true"><span class="octicon octicon-link"></span></a>put everything together<a class="anchor-link" href="#put-everything-together"> </a>
</h2>
<ul>
<li>위의 세 모듈을 한데 모아 구현</li>
<li>
<code>ignore_index</code>, <code>reduction</code> 추가 구현</li>
<li>설명은 시간이 되면 추가로 포스팅 업데이트</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">LogSoftmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="c1"># softmax(x) = softmax(x+c)</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">tensor</span> <span class="o">-</span> <span class="n">c</span>
        <span class="c1"># Calculate softmax</span>
        <span class="n">nominator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">denominator</span> <span class="o">=</span> <span class="n">nominator</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">nominator</span> <span class="o">/</span> <span class="n">denominator</span>
        <span class="c1"># Calculate log</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">log_probs</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="c1"># https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/SoftMax.cpp#L219</span>
        <span class="n">probs</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">grad_outputs</span> <span class="o">-=</span> <span class="n">probs</span> <span class="o">*</span> <span class="n">grad_outputs</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_outputs</span><span class="p">,</span> <span class="kc">None</span>


<span class="k">class</span> <span class="nc">NegativeLogLikelihoodLoss</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> 
                <span class="n">reduce</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"mean"</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="n">bsz</span><span class="p">,</span> <span class="n">n_classes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">ignore_index</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span>
            <span class="n">bsz</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> 
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">dim</span><span class="p">),</span> <span class="n">mask</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">({</span><span class="s2">"mean"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">"sum"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"none"</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">reduce</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ignore_index</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">dim_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bsz</span><span class="p">)</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">y</span>
        <span class="n">dim_y</span> <span class="o">=</span> <span class="n">y</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bsz</span><span class="p">)</span>
        <span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">dim_x</span><span class="p">,</span> <span class="n">dim_y</span><span class="p">]</span> <span class="c1"># Calculate Log Likelihood</span>
        <span class="n">nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_likelihood</span> <span class="c1"># Calculate Negative Log Likelihood</span>
        <span class="c1"># Calculate Loss</span>
        <span class="k">if</span> <span class="n">reduce</span> <span class="o">==</span> <span class="s2">"mean"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">nll</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span>
        <span class="k">elif</span> <span class="n">reduce</span> <span class="o">==</span> <span class="s2">"sum"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">nll</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span>
        <span class="n">nll</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.</span>
        <span class="k">return</span> <span class="n">nll</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="n">bsz</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="k">if</span> <span class="n">reduce</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span> <span class="c1"># reduce case</span>
            <span class="n">grad_outputs</span> <span class="o">=</span> <span class="n">grad_outputs</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">bsz</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">reduce</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># mean case</span>
            <span class="n">grad_outputs</span> <span class="o">=</span> <span class="n">grad_outputs</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">negative_mean_grad</span> <span class="o">=</span> <span class="o">-</span><span class="n">grad_outputs</span> <span class="c1"># backward negative</span>
        <span class="c1"># backward log likelihood (indexing)</span>
        <span class="k">if</span> <span class="n">dim</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">ll_grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">grad_outputs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">ll_grad</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bsz</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">ll_grad</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bsz</span><span class="p">),</span> <span class="n">ignore_index</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ll_grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">bsz</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">grad_outputs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">ll_grad</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bsz</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">ll_grad</span><span class="p">[</span><span class="n">ignore_index</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bsz</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">grad_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">negative_mean_grad</span><span class="p">)</span> <span class="o">@</span> <span class="n">ll_grad</span>
        <span class="k">return</span> <span class="n">grad_outputs</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>


<span class="k">class</span> <span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="n">log_softmax</span> <span class="o">=</span> <span class="n">LogSoftmax</span><span class="o">.</span><span class="n">apply</span>
    <span class="n">negative_log_likelihood</span> <span class="o">=</span> <span class="n">NegativeLogLikelihoodLoss</span><span class="o">.</span><span class="n">apply</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduce</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"mean"</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span> <span class="o">=</span> <span class="n">reduce</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span> <span class="o">=</span> <span class="n">ignore_index</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">y_pred</span><span class="p">:</span> <span class="n">_TensorOrTensors</span><span class="p">,</span> 
        <span class="n">y</span><span class="p">:</span> <span class="n">_TensorOrTensors</span><span class="p">,</span>
        <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_TensorOrTensors</span><span class="p">:</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="n">ce_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative_log_likelihood</span><span class="p">(</span>
            <span class="n">log_probs</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">ce_loss</span>

    <span class="k">def</span> <span class="nf">save_for_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">saved_tensors</span> <span class="o">=</span> <span class="n">args</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">:</span> <span class="n">_TensorOrTensors</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_TensorOrTensors</span><span class="p">:</span>
        <span class="n">probs</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bsz</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
        <span class="n">ce_grad</span> <span class="o">=</span> <span class="n">probs</span> <span class="o">-</span> <span class="n">y</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span> <span class="o">==</span> <span class="s2">"mean"</span><span class="p">:</span>
            <span class="n">ce_grad</span> <span class="o">=</span> <span class="n">ce_grad</span> <span class="o">/</span> <span class="n">bsz</span>
        <span class="k">return</span> <span class="n">grad_outputs</span> <span class="o">*</span> <span class="n">ce_grad</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Reference">
<a class="anchor" href="#Reference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reference<a class="anchor-link" href="#Reference"> </a>
</h1>
<ul>
<li><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/">https://machinelearningmastery.com/cross-entropy-for-machine-learning/</a></li>
<li><a href="https://stackoverflow.com/questions/61567597/how-is-log-softmax-implemented-to-compute-its-value-and-gradient-with-better">https://stackoverflow.com/questions/61567597/how-is-log-softmax-implemented-to-compute-its-value-and-gradient-with-better</a></li>
<li><a href="https://math.stackexchange.com/questions/1804805/how-is-the-entropy-of-the-normal-distribution-derived">https://math.stackexchange.com/questions/1804805/how-is-the-entropy-of-the-normal-distribution-derived</a></li>
<li><a href="https://datascienceschool.net/02%20mathematics/10.01%20%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC.html">https://datascienceschool.net/02%20mathematics/10.01%20%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC.html</a></li>
<li><a href="https://medium.com/konvergen/cross-entropy-and-maximum-likelihood-estimation-58942b52517a">https://medium.com/konvergen/cross-entropy-and-maximum-likelihood-estimation-58942b52517a</a></li>
<li><a href="https://discuss.pytorch.org/t/logsoftmax-vs-softmax/21386/4">https://discuss.pytorch.org/t/logsoftmax-vs-softmax/21386/4</a></li>
<li><a href="https://stackoverflow.com/questions/61567597/how-is-log-softmax-implemented-to-compute-its-value-and-gradient-with-better">https://stackoverflow.com/questions/61567597/how-is-log-softmax-implemented-to-compute-its-value-and-gradient-with-better</a></li>
<li><a href="https://github.com/pytorch/pytorch/issues/31829">https://github.com/pytorch/pytorch/issues/31829</a></li>
<li><a href="https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/SoftMax.cpp#L219">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/SoftMax.cpp#L219</a></li>
<li><a href="https://github.com/jinmang2/boostcamp_ai_tech_2/blob/main/u-stage/nlp/ch03_rnn/implement_rnn.py">https://github.com/jinmang2/boostcamp_ai_tech_2/blob/main/u-stage/nlp/ch03_rnn/implement_rnn.py</a></li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="jinmang2/jinmang2.github.io"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/implementation/ai-math/2022/02/07/cross-entropy.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/jinmang2" target="_blank" title="jinmang2"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
