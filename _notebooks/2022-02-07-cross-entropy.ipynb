{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy\n",
    "> PyTorchÏôÄ NumpyÎ°ú Íµ¨ÌòÑÌïòÎäî Cross Entropy\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Implementation, AI-math]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Ìò∏Ï∂ú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numbers\n",
    "from typing import Optional, Tuple, Sequence, Union, Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumpyÎ°ú Íµ¨ÌòÑÌïòÍ∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyCrossEntropy:\n",
    "\n",
    "    @staticmethod\n",
    "    def log_softmax(ndarray: np.ndarray, dim: int = -1) -> np.ndarray:\n",
    "        c = np.amax(ndarray, axis=dim, keepdims=True)\n",
    "        s = ndarray - c\n",
    "        nominator = np.exp(s)\n",
    "        denominator = nominator.sum(axis=-1, keepdims=True)\n",
    "        probs = nominator / denominator\n",
    "        return np.log(probs)\n",
    "\n",
    "    @staticmethod\n",
    "    def negative_log_likelihood(y_pred: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        # must to be batch_first\n",
    "        log_likelihood = y_pred[np.arange(y_pred.shape[0]), y]\n",
    "        nll = -log_likelihood\n",
    "        return np.mean(nll)\n",
    "\n",
    "    def cross_entropy(self, y_pred: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        log_probs = self.log_softmax(y_pred)\n",
    "        ce_loss = self.negative_log_likelihood(log_probs, y)\n",
    "        return ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nce = NumpyCrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorchÎ°ú Íµ¨ÌòÑÌïòÍ∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TensorOrTensors = Union[torch.Tensor, Sequence[torch.Tensor]]\n",
    "\n",
    "\n",
    "class LogSoftmax(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx: Any, tensor: Any, dim: int = -1) -> Any:\n",
    "        # softmax(x) = softmax(x+c)\n",
    "        c = torch.amax(tensor, dim=dim, keepdims=True)\n",
    "        s = tensor - c\n",
    "        # Calculate softmax\n",
    "        nominator = torch.exp(s)\n",
    "        denominator = nominator.sum(dim=dim, keepdims=True)\n",
    "        probs = nominator / denominator\n",
    "        # Calculate log\n",
    "        log_probs = torch.log(probs)\n",
    "        ctx.save_for_backward(probs, torch.tensor(dim))\n",
    "        return log_probs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, grad_outputs: Any) -> Any:\n",
    "        # https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/SoftMax.cpp#L219\n",
    "        probs, dim, = ctx.saved_tensors\n",
    "        grad_outputs -= probs * grad_outputs.sum(dim=dim.item(), keepdims=True)\n",
    "        return grad_outputs, None\n",
    "\n",
    "\n",
    "class NegativeLogLikelihoodLoss(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx: Any, y_pred: Any, y: Any) -> Any:\n",
    "        bsz, n_classes = torch.tensor(y_pred.size())\n",
    "        ctx.save_for_backward(bsz, n_classes, y)\n",
    "        log_likelihood = y_pred[torch.arange(bsz), y] # Calculate Log Likelihood\n",
    "        nll = -log_likelihood # Calculate Negative Log Likelihood\n",
    "        return torch.mean(nll) # Calculate Loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, grad_outputs: Any) -> Any:\n",
    "        bsz, n_classes, y, = ctx.saved_tensors\n",
    "        mean_grad = grad_outputs.expand(bsz) / bsz # backward mean function\n",
    "        negative_mean_grad = -mean_grad # backward negative\n",
    "        # backward log likelihood (indexing)\n",
    "        ll_grad = torch.zeros(bsz, n_classes)\n",
    "        ll_grad[torch.arange(bsz), y] = 1.\n",
    "        grad_outputs = torch.diag(negative_mean_grad) @ ll_grad\n",
    "        return grad_outputs, None\n",
    "\n",
    "\n",
    "class CrossEntropyLoss(nn.Module):\n",
    "    \n",
    "    log_softmax = LogSoftmax.apply\n",
    "    negative_log_likelihood = NegativeLogLikelihoodLoss.apply\n",
    "\n",
    "    def forward(self, y_pred, y):\n",
    "        log_probs = self.log_softmax(y_pred, -1)\n",
    "        ce_loss = self.negative_log_likelihood(log_probs, y)\n",
    "        probs = torch.exp(log_probs) / log_probs.size(0)\n",
    "        self.save_for_backward(probs, y, y_pred.size(0), y_pred.size(-1))\n",
    "        return ce_loss\n",
    "\n",
    "    def save_for_backward(self, *args):\n",
    "        self.saved_tensors = args\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def backward(self, grad_outputs: _TensorOrTensors) -> _TensorOrTensors:\n",
    "        probs, y, bsz, num_classes, = self.saved_tensors\n",
    "        y = torch.nn.functional.one_hot(y, num_classes=num_classes)\n",
    "        ce_grad = probs - (y / bsz)\n",
    "        return grad_outputs * ce_grad\n",
    "    \n",
    "    \n",
    "class PyTorchCrossEntropy:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cross_entropy = CrossEntropyLoss()\n",
    "        self.log_softmax = self.cross_entropy.log_softmax\n",
    "        self.negative_log_likelihood = self.cross_entropy.negative_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "tce = PyTorchCrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Í≤∞Í≥ºÍ∞í ÎπÑÍµê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "vocab_size = 3000\n",
    "\n",
    "rtol = 1e-4\n",
    "atol = 1e-6\n",
    "isclose = partial(torch.isclose, rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [[random.normalvariate(mu=0., sigma=1.) for _ in range(vocab_size)] for _ in range(batch_size)]\n",
    "y_pred_torch = torch.FloatTensor(y_pred)\n",
    "y_pred_torch.requires_grad = True\n",
    "y_pred_numpy = y_pred_torch.detach().numpy()\n",
    "\n",
    "y = [random.randint(0, vocab_size) for _ in range(batch_size)]\n",
    "y_torch = torch.LongTensor(y)\n",
    "y_numpy = y_torch.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do both output the same tensors? üî•\n"
     ]
    }
   ],
   "source": [
    "ce_result = nn.CrossEntropyLoss()(y_pred_torch, y_torch)\n",
    "ce_numpy = nce.cross_entropy(y_pred_numpy, y_numpy)\n",
    "ce_torch = tce.cross_entropy(y_pred_torch, y_torch)\n",
    "\n",
    "try:\n",
    "    isclose(ce_result, ce_torch).item()\n",
    "    isclose(ce_result, torch.tensor(ce_numpy)).item()\n",
    "    success = True\n",
    "except:\n",
    "    success = False\n",
    "\n",
    "print(\"Do both output the same tensors?\", \"üî•\" if success else \"üí©\")\n",
    "if not success:\n",
    "    raise Exeption(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do both output the same tensors? üî•\n"
     ]
    }
   ],
   "source": [
    "# backward (under debugging)\n",
    "ce_grad = torch.autograd.grad(ce_result, y_pred_torch, retain_graph=True)[0]\n",
    "my_ce_grad1 = torch.autograd.grad(ce_torch, y_pred_torch, retain_graph=True)[0]\n",
    "my_ce_grad2 = tce.cross_entropy.backward(torch.ones_like(y_pred_torch))\n",
    "# my_ce_grad2 = tce.cross_entropy.backward(y_pred_torch)\n",
    "\n",
    "try:\n",
    "    isclose(ce_grad, my_ce_grad1).all()\n",
    "    isclose(ce_grad, my_ce_grad2).all()\n",
    "    success = True\n",
    "except:\n",
    "    success = False\n",
    "\n",
    "print(\"Do both output the same tensors?\", \"üî•\" if success else \"üí©\")\n",
    "if not success:\n",
    "    raise Exeption(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~isclose(ce_grad, my_ce_grad)).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fe6c8e22e8>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot((~isclose(ce_grad, my_ce_grad)).sum(dim=0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~isclose(ce_grad, my_ce_grad)[:, 1000:2000]).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
