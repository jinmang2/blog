{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy\n",
    "> PyTorch와 Numpy로 구현하는 Cross Entropy\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Implementation, AI-math]\n",
    "- image: images/cross_entropy_fig1.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "본 포스팅에서는 Cross Entropy에 대해 알아보겠습니다.\n",
    "\n",
    "- Cross Entropy의 개념\n",
    "- Cross Entropy 수식\n",
    "- Matplotlib으로 figure 그려보기\n",
    "- Numpy로 코드 구현체 살펴보기\n",
    "- PyTorch로 forward, backward pass 수식과 함께 이해하기\n",
    "- PyTorch 결과값과 동일한 출력이 나오도록 코드 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy는 random variable(확률 변수) 혹은 set of events(사건 집합)이 주어졌을 때 서로 다른 두 확률 분포 사이의 차이를 재는 measure입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "\n",
    "Event $x$가 등장할 확률을 $P(x)$라고 두고 이에 대한 정보량 $h(x)$에 대한 수식은 아래와 같이 정의됩니다.\n",
    "\n",
    "$$h(x)=-\\log{P(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\log$ 함수의 그래프에 음수를 씌우면 아래와 같이 그릴 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(0.01, 1, 100)\n",
    "y = - np.log(x)\n",
    "plt.axvline(0, color=\"k\", alpha=0.7)\n",
    "plt.axhline(0, color=\"k\", alpha=0.7)\n",
    "plt.plot(x, y, lw=3)\n",
    "plt.xlabel(\"probability\")\n",
    "plt.ylabel(\"information\")\n",
    "plt.grid()\n",
    "_ = plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그래프로부터 아래의 사실을 확인할 수 있습니다.\n",
    "- $P(x)$가 `0.0`에 가까울 때 (사건이 등장할 확률이 희박할 때) 해당 사건에 대한 정보량이 높음을 알 수 있습니다.\n",
    "- $P(x)$가 `1.0`에 가까울 때 (사건이 등장할 확률이 높을 때) 해당 사건에 대한 정보량이 낮음을 알 수 있습니다.\n",
    "\n",
    "때문에 이를 정보 이론에서는 `surprise` 라고 묘사합니다. 우리가 매일 마주하는 일상에서는 크게 놀랄만한 정보가 없습니다. 그러나 취업을 했달지 연애를 시작한다던지 인생에 드물게 찾아오는 사건이 생기면 우리는 이 기쁨에 취하며 놀라곤 합니다.\n",
    "\n",
    "즉, 드물게 발생할 수록 더욱 정보량이 높은 것이지요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단일 사건에 대한 정보량말고 set of $x$, $X$를 생각해보겠습니다. $X$는 이산 확률 분포라고 가정할게요. 그러면 다음과 같이 식을 적을 수 있습니다.\n",
    "\n",
    "$$H(X)=-\\sum_{x\\in X}P(x)\\cdot\\log{P(x)}$$\n",
    "\n",
    "$X$의 모든 사건 $x$에 대한 정보량들을 더한 값을 $X$에 대한 정보량으로 생각할 수 있다라고 식이 적혀있습니다. Entropy는 가능한 모든 사건이 발생할 확률이 전부 같을 때 최댓값을 가집니다. 각각의 정보량은 발생할 확률이 작을수록 값이 커지기 때문이죠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이산변수의 Entropy\n",
    "\n",
    "Discrete variable의 기댓값은 summation으로 쓸 수 있습니다. 위의 수식을 잘 살펴보죠. 기댓값의 수식이죠?\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\\\\\n",
    "H(X)&=-\\sum_{x\\in X}P(x)\\cdot\\log{P(x)}\\\\\n",
    "&=\\sum_{x\\in X}P(x)\\cdot(-\\log{P(x)})\\\\\n",
    "&=\\mathbb{E}\\big[-\\log{P(x)}\\big]\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "위 수식을 잘 기억해주세요. 이산변수는 기댓값을 합으로 계산하기 때문에 아래와 같이 쉽게 계산할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률 분포 Y가 이산분포이고 다음과 같다고 가정해보겠습니다.\n",
    "- 확률 분포 $Y_1$: $P(Y=0)=0.8,\\;P(Y=1)=0.2$\n",
    "\n",
    "$Y_1$에 대한 entropy를 정의된 식과 같이 동일하게 계산할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7219280948873623"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_y = [0.8, 0.2]\n",
    "\n",
    "sum([p * -np.log2(p) for p in p_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연속변수의 Entropy\n",
    "\n",
    "이와는 다르게 Continuous variable의 기댓값은 Integral로 쓸 수 있습니다.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\\\\\n",
    "H(X)&=\\mathbb{E}\\big[-\\log{P(X)}\\big]\\\\\n",
    "&=\\int_{-\\infty}^{\\infty}{P(X)\\cdot(-\\log{P(X)})}dx\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "위를 trapezoidal rule을 사용하여 적분값을 계산해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normal(x, mu, sigma):\n",
    "    var = sigma ** 2\n",
    "    x = x - mu\n",
    "    return (1 / np.sqrt(2 * np.pi * var)) * np.exp(-x**2 / (2 * var))\n",
    "\n",
    "entropy = lambda p: -p * np.log(p)\n",
    "\n",
    "def trapezoidal_rule(dt, p, f):\n",
    "    return np.sum((f(p[:-1]) + f(p[1:])) * dt) / 2\n",
    "\n",
    "xlim, ylim, n_sample, n_bin = 10, 0.5, 50000, 1000\n",
    "yticks = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "mu1, mu2 = 0.0, 0.0\n",
    "sigma1, sigma2 = 1.0, 2.5\n",
    "x = np.linspace(-xlim, xlim, n_sample)\n",
    "X1 = np.random.normal(loc=mu1, scale=sigma1, size=(n_sample))\n",
    "X2 = np.random.normal(loc=mu2, scale=sigma2, size=(n_sample))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "def plot_normal_entropy(ax, X, mu, sigma):\n",
    "    _, bins, _ = ax.hist(X, bins=n_bin, density=True, histtype=\"stepfilled\", \n",
    "                         color=\"slateblue\", edgecolor=\"k\", lw=0.1)\n",
    "    ax.plot(x, normal(x, mu, sigma), color=\"k\", lw=2)\n",
    "    ax.set_ylim(0, ylim)\n",
    "    ax.set_yticks(yticks)\n",
    "    dt = np.diff(bins)\n",
    "    p = normal(bins, mu, sigma)\n",
    "    ent = trapezoidal_rule(dt, p, entropy)\n",
    "    ax.set_title(rf\"N(${mu}$, ${sigma}^2$)  H(p)={ent:.4f}\")\n",
    "\n",
    "plot_normal_entropy(fig.add_subplot(1, 2, 1), X1, mu1, sigma1)\n",
    "plt.ylabel(\"probability\")\n",
    "plot_normal_entropy(fig.add_subplot(1, 2, 2), X2, mu2, sigma2)\n",
    "_ = plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위처럼 직접 적분값을 근사하여 계산할 수도 있지만 정규 분포 엔트로피 값을 해석적으로도 계산할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규 분포의 수식은 아래와 같습니다.\n",
    "\n",
    "$$P(X)=\\cfrac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{\\bigg(\\cfrac{-(X-\\mu)^2}{2\\sigma^2}\\bigg)}$$\n",
    "\n",
    "이제 엔트로피 수식을 전개해봅시다.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\\\\\n",
    "H(X)&=\\mathbb{E}\\big[-\\log{P(X)}\\big]\\\\\n",
    "&=\\int_{-\\infty}^{\\infty}{P(X)\\cdot(-\\log{P(X)})}dX\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "여기서 $-\\log P(X)$는 $\\cfrac{1}{2}\\log{2\\pi\\sigma^2}+\\cfrac{1}{2\\sigma^2}(X-\\mu)^2$이고 이를 대입하면\n",
    "\n",
    "$$\n",
    "H(X)=\\cfrac{1}{2}\\log{2\\pi\\sigma^2}\\int_{-\\infty}^{\\infty}{P(X)}dX+\\cfrac{1}\n",
    "{2\\sigma^2}\\int_{-\\infty}^{\\infty}(X-\\mu)^2P(X)dX\n",
    "$$\n",
    "\n",
    "여기서 확률 및 분산의 정의에 따라 다음을 알 수 있고\n",
    "\n",
    "$$\\int_{-\\infty}^{\\infty}{P(X)}dX=1$$\n",
    "$$\\int_{-\\infty}^{\\infty}(X-\\mu)^2P(X)dX=\\sigma^2$$\n",
    "\n",
    "이를 사용하여 식을 다시 작성하면\n",
    "\n",
    "$$H(X)=\\cfrac{1}{2}\\big(\\log{2\\pi\\sigma^2}+1\\big)$$\n",
    "\n",
    "와 같이 쓸 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python code로 구현하여 위의 결과와 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4189385332046727"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normal_entropy(sigma):\n",
    "    # return 0.5 * np.log(np.e * 2 * np.pi * sigma**2)\n",
    "    return 0.5 * (1 + np.log(2 * np.pi * sigma**2))\n",
    "\n",
    "normal_entropy(sigma1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "\n",
    "Machine Learning: A Probabilistic Perspective에서는 cross entropy를 다음과 같이 설명합니다.\n",
    "\n",
    "```\n",
    "The cross entropy is the average number of bits needed to encode data coming from a source with distribution p when we use model q ...\n",
    "```\n",
    "\n",
    "$P$는 모델링하고자 하는 분포, $Q$는 모델링을 할 분포라고 생각하면 이해가 빠릅니다. $P$ 대신 $Q$를 사용하여 사건을 나타내는 총 비트의 평균 수 입니다.\n",
    "\n",
    "$$H(P,Q)=-\\sum_{x\\in X}P(x)\\cdot\\log Q(x)$$\n",
    "\n",
    "총 비트 수가 아니라 추가로 필요한 비트의 평균값은? `relative entropy`, `Kullback-Leibler Divergence` 라고 합니다.\n",
    "\n",
    "$$KL(P||Q)=-\\sum_{x\\in X}P(x)\\cdot\\log{\\cfrac{Q(x)}{P(x)}}$$\n",
    "\n",
    "위 수식은 Entropy와 엮어서 다음과 같이 쓸 수 있습니다.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\\\\\n",
    "H(P)&=\\sum_{x \\in X} P(x)\\cdot\\log P(x)\\\\\n",
    "&=H(P,Q)-KL(P||Q)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy and Maximum Likelihood Estimation\n",
    "\n",
    "분류 문제를 풀기 위해 Neural Network를 학습시킬 때, 우리는 흔히 Cross Entropy로 학습시킵니다. 왜일까요?\n",
    "\n",
    "위에서 Entropy, Cross Entropy, KL-Divergence에 대한 수식을 정의했습니다.\n",
    "\n",
    "앞서 확률 변수의 Entropy 정의에서 Entropy가 확률 변수의 Expectation과 관련이 있음을 확인했었습니다. 각각을 기댓값 표현으로 써보면,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\\\\\n",
    "H(X)&=-\\sum_x p(x)\\log p(x)\\\\\n",
    "&=\\sum_x -p(x)\\log p(x)\\\\\n",
    "&=\\sum_x p(x)\\log \\cfrac{1}{p(x)}\\\\\n",
    "&=\\mathbb{E}_{X\\sim p(x)}\\bigg[\\log \\cfrac{1}{p(x)}\\bigg]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\\\\\n",
    "KL(p||q)&=\\sum_x p(x)\\log \\cfrac{p(x)}{q(x)}\\\\\n",
    "&=\\mathbb{E}_{X\\sim p(x)}\\bigg[\\log\\cfrac{p(x)}{q(x)}\\bigg]\\\\\n",
    "&=\\mathbb{E}_{X\\sim p(x)}\\bigg[\\log\\cfrac{1}{q(x)}-\\cfrac{1}{p(x)}\\bigg]\\\\\n",
    "&=\\mathbb{E}_{X\\sim p(x)}\\bigg[\\log\\cfrac{1}{q(x)}\\bigg]-\\mathbb{E}_{X\\sim p(x)}\\bigg[\\log\\cfrac{1}{p(x)}\\bigg]\\\\\n",
    "&=\\mathbb{E}_{X\\sim p(x)}\\bigg[\\log\\cfrac{1}{q(x)}\\bigg] - H(p)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Cross entropy는 아래와 같이 적을 수 있죠.\n",
    "\n",
    "$$H(p,q)=\\mathbb{E}_{X\\sim p(x)}\\bigg[\\log\\cfrac{1}{q(x)}\\bigg]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 Maximum Likelihood Estimation에서의 objective function을 살펴보겠습니다.\n",
    "\n",
    "우리는 분포 $p$를 모델링하고 싶습니다. 이에 대한 데이터 $X$를 모수 $\\theta$를 가지는 parametric model $q$로 모델링하면 아래와 같이 수식을 쓸 수 있습니다.\n",
    "\n",
    "$$\\theta_{ML}=\\underset{\\theta}{\\mathrm{argmax}}\\;q(X;\\theta)$$\n",
    "\n",
    "i.i.d 가정에 의해 아래와 같이 수식을 작성하면 (로그 스케일로)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\\\\\n",
    "\\theta_{ML}&=\\underset{\\theta}{\\mathrm{argmax}}\\;\\prod_{i=1}^{m}q(x_i;\\theta)\\\\\n",
    "&=\\underset{\\theta}{\\mathrm{argmax}}\\;\\sum_{i=1}^{m}\\log q(x_i;\\theta)\\\\\n",
    "&=\\underset{\\theta}{\\mathrm{argmax}}\\;\\sum_{i=1}^{m}\\frac{1}{m}\\log q(x_i;\\theta)\\\\\n",
    "&=\\underset{\\theta}{\\mathrm{argmax}}\\;\\mathbb{E}_{X\\sim p(x)}\\big[\\log q(x)\\big]\\\\\n",
    "&=\\underset{\\theta}{\\mathrm{argmin}}\\;-\\mathbb{E}_{X\\sim p(x)}\\big[\\log q(x)\\big]\\\\\n",
    "&=\\underset{\\theta}{\\mathrm{argmin}}\\;\\mathbb{E}_{X\\sim p(x)}\\big[\\log\\frac{1}{q(x)}\\big]\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "즉, Likelihood를 maximize하는 문제는 Cross Entropy를 Minimize하는 문제로 치환할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Implementation\n",
    "\n",
    "Cross Entropy를 구현해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numbers\n",
    "from typing import Optional, Tuple, Sequence, Union, Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "_TensorOrTensors = Union[torch.Tensor, Sequence[torch.Tensor]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy로 구현하기: forward\n",
    "\n",
    "Cross Entropy는 `LogSoftmax`와 `NegativeLogLikelihood`로 계산할 수 있습니다. 위에서 본 것처럼 우도를 최대화하는 문제와 Cross entropy를 최소화하는 문제는 동일하기 때문에 이를 Negative Log likelihood를 최소화하는 문제로 생각해도 무방하겠죠? 이를 활용하여 Cross entropy를 구해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 가지, 구현 테크닉을 소개드리고자 합니다. softmax 함수는 다음과 같은 특징을 가지고 있습니다.\n",
    "\n",
    "$$\\mathrm{softmax}(x)=\\mathrm{softmax}(x+c)$$\n",
    "\n",
    "이를 활용하여 overflow 문제를 해결할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathrm{softmax}(x_i)=\\cfrac{exp(x_i)}{\\sum_j exp(x_j)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax_numpy(arr):\n",
    "    c = np.amax(arr, axis=-1, keepdims=True)\n",
    "    s = arr - c\n",
    "    nominator = np.exp(s)\n",
    "    denominator = nominator.sum(axis=-1, keepdims=True)\n",
    "    probs = nominator / denominator\n",
    "    return np.log(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우도는 fancy indexing으로 간단하게 계산할 수 있습니다. 구한 likelihood에 음수를 씌워주면 Negative Likelihood가 되겠지요.\n",
    "\n",
    "구한 Negative Log Likelihood는 평균값으로 reduce하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood_numpy(y_pred, y):\n",
    "    log_likelihood = y_pred[np.arange(y_pred.shape[0]), y]\n",
    "    nll = -log_likelihood\n",
    "    return nll.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy는 이제 간단합니다. 예측 값 Q에 log softmax를 취해주고 음의 로그 가능도를 계산해주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_numpy(y_pred, y):\n",
    "    log_probs = log_softmax_numpy(y_pred)\n",
    "    ce_loss = negative_log_likelihood_numpy(log_probs, y)\n",
    "    return ce_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch로 구현하기: forward\n",
    "\n",
    "이를 pytorch로도 구현해봅시다. 구현은 메서드명까지 동일합니다. 차이라면 numpy에서는 차원축을 axis라는 parameter로 받는 반면, torch는 dim이라는 parameter로 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax_torch(tensor):\n",
    "    c = torch.amax(tensor, dim=-1, keepdims=True)\n",
    "    s = tensor - c\n",
    "    nominator = torch.exp(s)\n",
    "    denominator = nominator.sum(dim=-1, keepdims=True)\n",
    "    probs = nominator / denominator\n",
    "    return torch.log(probs)\n",
    "\n",
    "\n",
    "def negative_log_likelihood_torch(y_pred, y):\n",
    "    log_likelihood = y_pred[torch.arange(y_pred.shape[0]), y]\n",
    "    nll = -log_likelihood\n",
    "    return nll.mean()\n",
    "\n",
    "\n",
    "def cross_entropy_torch(y_pred, y):\n",
    "    log_probs = log_softmax_torch(y_pred)\n",
    "    ce_loss = negative_log_likelihood_torch(log_probs, y)\n",
    "    return ce_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward 결과값 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import partial\n",
    "\n",
    "batch_size = 8\n",
    "vocab_size = 3000\n",
    "\n",
    "rtol = 1e-4\n",
    "atol = 1e-6\n",
    "isclose = partial(torch.isclose, rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [\n",
    "    [random.normalvariate(mu=0., sigma=1.) for _ in range(vocab_size)]\n",
    "    for _ in range(batch_size)\n",
    "]\n",
    "y_pred_torch = torch.FloatTensor(y_pred)\n",
    "y_pred_torch.requires_grad = True\n",
    "y_pred_numpy = y_pred_torch.detach().numpy()\n",
    "\n",
    "y = [random.randint(0, vocab_size) for _ in range(batch_size)]\n",
    "y_torch = torch.LongTensor(y)\n",
    "y_numpy = y_torch.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do both output the same tensors? 🔥\n"
     ]
    }
   ],
   "source": [
    "ce_result = nn.CrossEntropyLoss()(y_pred_torch, y_torch)\n",
    "ce_numpy = cross_entropy_numpy(y_pred_numpy, y_numpy)\n",
    "ce_torch = cross_entropy_torch(y_pred_torch, y_torch)\n",
    "\n",
    "try:\n",
    "    isclose(ce_result, ce_torch).item()\n",
    "    isclose(ce_result, torch.tensor(ce_numpy)).item()\n",
    "    success = True\n",
    "except:\n",
    "    success = False\n",
    "\n",
    "print(\"Do both output the same tensors?\", \"🔥\" if success else \"💩\")\n",
    "if not success:\n",
    "    raise Exeption(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch로 구현하기: backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Softmax\n",
    "\n",
    "$$o(1-o)$$\n",
    "- 1은 크로네클 델타\n",
    "- log softmax가 계산 상 이점이 큼\n",
    "- loss를 더 크게 만들어 주기도\n",
    "- softmax의 backward의 grad_outputs에 log 함수의 역함수인 1/x를 넣어주면 log_softmax의 backward form이 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _softmax(tensor: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
    "    c = torch.amax(tensor, dim=dim, keepdims=True)\n",
    "    s = tensor - c\n",
    "    nominator = torch.exp(s)\n",
    "    denominator = nominator.sum(dim=dim, keepdims=True)\n",
    "    probs = nominator / denominator\n",
    "    return probs\n",
    "\n",
    "\n",
    "class Softmax(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx: Any, tensor: Any, dim: int = -1) -> Any:\n",
    "        probs = _softmax(tensor)\n",
    "        ctx.save_for_backward(probs, torch.tensor(dim))\n",
    "        return probs\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, grad_outputs: Any) -> Any:\n",
    "        probs, dim, = ctx.saved_tensors\n",
    "        grad_outputs -= (grad_outputs * probs).sum(dim.item(), keepdims=True)\n",
    "        return probs * grad_outputs, None\n",
    "\n",
    "\n",
    "softmax = Softmax.apply\n",
    "\n",
    "\n",
    "class LogSoftmax(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx: Any, tensor: Any, dim: int = -1) -> Any:\n",
    "        probs = _softmax(tensor)\n",
    "        ctx.save_for_backward(probs, torch.tensor(dim))\n",
    "        return torch.log(probs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, grad_outputs: Any) -> Any:\n",
    "        probs, dim, = ctx.saved_tensors\n",
    "        grad_outputs -= probs * grad_outputs.sum(dim=dim.item(), keepdims=True)\n",
    "        return grad_outputs, None\n",
    "    \n",
    "\n",
    "log_softmax = LogSoftmax.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 3, requires_grad=True)\n",
    "\n",
    "# Softmax check\n",
    "# -- forward pass\n",
    "y_orig = nn.functional.softmax(x, dim=-1)\n",
    "y_impl = y = softmax(x, -1)\n",
    "assert isclose(y_orig, y_impl).all(), \"💩\"\n",
    "# -- backward pass\n",
    "dy_orig = torch.autograd.grad(y_orig, x, torch.ones_like(x), retain_graph=True)[0]\n",
    "dy_impl = torch.autograd.grad(y_impl, x, torch.ones_like(x), retain_graph=True)[0]\n",
    "assert isclose(dy_orig, dy_impl).all(), \"💩\"\n",
    "\n",
    "# LogSoftmax check\n",
    "# -- forward pass\n",
    "y_orig = nn.functional.log_softmax(x, dim=-1)\n",
    "y_impl = y = log_softmax(x, -1)\n",
    "assert isclose(y_orig, y_impl).all(), \"💩\"\n",
    "# -- backward pass\n",
    "dy_orig = torch.autograd.grad(y_orig, x, torch.ones_like(x), retain_graph=True)[0]\n",
    "dy_impl = torch.autograd.grad(y_impl, x, torch.ones_like(x), retain_graph=True)[0]\n",
    "assert isclose(dy_orig, dy_impl).all(), \"💩\"\n",
    "\n",
    "# Log + Softmax check\n",
    "# -- forward pass\n",
    "y1 = torch.log(softmax(x, -1))\n",
    "y2 = log_softmax(x, -1)\n",
    "assert isclose(y1, y2).all(), \"💩\"\n",
    "# -- backward pass\n",
    "dy1 = torch.autograd.grad(y1, x, torch.ones_like(x), retain_graph=True)[0]\n",
    "dy2 = torch.autograd.grad(y2, x, torch.ones_like(x), retain_graph=True)[0]\n",
    "assert isclose(dy1, dy2).all(), \"💩\"\n",
    "\n",
    "print(\"🔥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeLogLikelihoodLoss(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx: Any, y_pred: Any, y: Any) -> Any:\n",
    "        bsz, n_classes = torch.tensor(y_pred.size())\n",
    "        ctx.save_for_backward(bsz, n_classes, y)\n",
    "        log_likelihood = y_pred[torch.arange(bsz), y]\n",
    "        nll = -log_likelihood\n",
    "        return nll.mean()\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, grad_outputs: Any) -> Any:\n",
    "        bsz, n_classes, y, = ctx.saved_tensors\n",
    "        grad_outputs = grad_outputs.expand(bsz) / bsz\n",
    "        negative_grad = -grad_outputs\n",
    "        ll_grad = torch.zeros(bsz, n_classes, device=grad_outputs.device)\n",
    "        ll_grad[torch.arange(bsz), y] = 1.\n",
    "        grad_outputs = torch.diag(negative_grad) @ ll_grad\n",
    "        return grad_outputs, None\n",
    "    \n",
    "nll_loss = NegativeLogLikelihoodLoss.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(nn.Module):\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        y_pred: _TensorOrTensors, \n",
    "        y: _TensorOrTensors\n",
    "    ) -> _TensorOrTensors:\n",
    "        log_probs = log_softmax(y_pred)\n",
    "        ce_loss = nll_loss(log_probs, y)\n",
    "        probs = torch.exp(log_probs) / log_probs.size(0)\n",
    "        self.save_for_backward(probs, y, y_pred.size(-1))\n",
    "        return ce_loss\n",
    "    \n",
    "    def save_for_backward(self, *args):\n",
    "        self.saved_tensors = args\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def backward(self, grad_outputs: _TensorOrTensors) -> _TensorOrTensors:\n",
    "        probs, y, num_classes, = self.saved_tensors\n",
    "        ce_grad = probs - torch.nn.functional.one_hot(y, num_classes=num_classes)\n",
    "        return grad_outputs * ce_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## put everything together\n",
    "- 위의 세 모듈을 한데 모아 구현\n",
    "- `ignore_index`, `reduction` 추가 구현\n",
    "- 설명은 시간이 되면 추가로 포스팅 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogSoftmax(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx: Any, tensor: Any, dim: int = -1) -> Any:\n",
    "        # softmax(x) = softmax(x+c)\n",
    "        c = torch.amax(tensor, dim=dim, keepdims=True)\n",
    "        s = tensor - c\n",
    "        # Calculate softmax\n",
    "        nominator = torch.exp(s)\n",
    "        denominator = nominator.sum(dim=dim, keepdims=True)\n",
    "        probs = nominator / denominator\n",
    "        # Calculate log\n",
    "        log_probs = torch.log(probs)\n",
    "        ctx.save_for_backward(probs, torch.tensor(dim))\n",
    "        return log_probs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, grad_outputs: Any) -> Any:\n",
    "        # https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/SoftMax.cpp#L219\n",
    "        probs, dim, = ctx.saved_tensors\n",
    "        grad_outputs -= probs * grad_outputs.sum(dim=dim.item(), keepdims=True)\n",
    "        return grad_outputs, None\n",
    "\n",
    "\n",
    "class NegativeLogLikelihoodLoss(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx: Any, y_pred: Any, y: Any, dim: int = -1, \n",
    "                reduce: str = \"mean\", ignore_index: int = -1) -> Any:\n",
    "        bsz, n_classes = torch.tensor(y_pred.size())\n",
    "        mask = y.ne(ignore_index)\n",
    "        ctx.save_for_backward(\n",
    "            bsz, n_classes, y, \n",
    "            torch.tensor(dim), mask,\n",
    "            torch.tensor({\"mean\": 0, \"sum\": 1, \"none\": 2}.get(reduce, -1)),\n",
    "            torch.tensor(ignore_index)\n",
    "        )\n",
    "        dim_x = torch.arange(bsz) if dim != 0 else y\n",
    "        dim_y = y if dim != 0 else torch.arange(bsz)\n",
    "        log_likelihood = y_pred[dim_x, dim_y] # Calculate Log Likelihood\n",
    "        nll = -log_likelihood # Calculate Negative Log Likelihood\n",
    "        # Calculate Loss\n",
    "        if reduce == \"mean\":\n",
    "            return torch.mean(nll[mask])\n",
    "        elif reduce == \"sum\":\n",
    "            return torch.sum(nll[mask])\n",
    "        nll[~mask] = 0.\n",
    "        return nll\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, grad_outputs: Any) -> Any:\n",
    "        bsz, n_classes, y, dim, mask, reduce, ignore_index, = ctx.saved_tensors\n",
    "        if reduce.item() != 2: # reduce case\n",
    "            grad_outputs = grad_outputs.expand(bsz)\n",
    "        if reduce.item() == 0: # mean case\n",
    "            grad_outputs = grad_outputs / mask.sum()\n",
    "        negative_mean_grad = -grad_outputs # backward negative\n",
    "        # backward log likelihood (indexing)\n",
    "        if dim.item() != 0:\n",
    "            ll_grad = torch.zeros(bsz, n_classes, device=grad_outputs.device)\n",
    "            ll_grad[torch.arange(bsz), y] = 1\n",
    "            ll_grad[torch.arange(bsz), ignore_index.item()] = 0\n",
    "        else:\n",
    "            ll_grad = torch.zeros(n_classes, bsz, device=grad_outputs.device)\n",
    "            ll_grad[y, torch.arange(bsz)] = 1\n",
    "            ll_grad[ignore_index.item(), torch.arange(bsz)] = 0\n",
    "        grad_outputs = torch.diag(negative_mean_grad) @ ll_grad\n",
    "        return grad_outputs, None, None, None, None\n",
    "\n",
    "\n",
    "class CrossEntropyLoss(nn.Module):\n",
    "    \n",
    "    log_softmax = LogSoftmax.apply\n",
    "    negative_log_likelihood = NegativeLogLikelihoodLoss.apply\n",
    "    \n",
    "    def __init__(self, reduce: str = \"mean\", ignore_index: int = -1):\n",
    "        super().__init__()\n",
    "        self.reduce = reduce\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        y_pred: _TensorOrTensors, \n",
    "        y: _TensorOrTensors,\n",
    "        dim: int = -1,\n",
    "    ) -> _TensorOrTensors:\n",
    "        log_probs = self.log_softmax(y_pred, dim)\n",
    "        ce_loss = self.negative_log_likelihood(\n",
    "            log_probs, y, dim, self.reduce, self.ignore_index)\n",
    "        probs = torch.exp(log_probs)\n",
    "        self.save_for_backward(probs, y, y_pred.size(0), y_pred.size(-1))\n",
    "        return ce_loss\n",
    "\n",
    "    def save_for_backward(self, *args):\n",
    "        self.saved_tensors = args\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def backward(self, grad_outputs: _TensorOrTensors) -> _TensorOrTensors:\n",
    "        probs, y, bsz, num_classes, = self.saved_tensors\n",
    "        y = torch.nn.functional.one_hot(y, num_classes=num_classes)\n",
    "        ce_grad = probs - y\n",
    "        if self.reduce == \"mean\":\n",
    "            ce_grad = ce_grad / bsz\n",
    "        return grad_outputs * ce_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- https://machinelearningmastery.com/cross-entropy-for-machine-learning/\n",
    "- https://stackoverflow.com/questions/61567597/how-is-log-softmax-implemented-to-compute-its-value-and-gradient-with-better\n",
    "- https://math.stackexchange.com/questions/1804805/how-is-the-entropy-of-the-normal-distribution-derived\n",
    "- https://datascienceschool.net/02%20mathematics/10.01%20%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC.html\n",
    "- https://medium.com/konvergen/cross-entropy-and-maximum-likelihood-estimation-58942b52517a\n",
    "- https://discuss.pytorch.org/t/logsoftmax-vs-softmax/21386/4\n",
    "- https://stackoverflow.com/questions/61567597/how-is-log-softmax-implemented-to-compute-its-value-and-gradient-with-better\n",
    "- https://github.com/pytorch/pytorch/issues/31829\n",
    "- https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/SoftMax.cpp#L219\n",
    "- https://github.com/jinmang2/boostcamp_ai_tech_2/blob/main/u-stage/nlp/ch03_rnn/implement_rnn.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
