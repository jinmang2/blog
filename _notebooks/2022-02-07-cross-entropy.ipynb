{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy\n",
    "> PyTorchÏôÄ NumpyÎ°ú Íµ¨ÌòÑÌïòÎäî Cross Entropy\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Implementation, AI-math]\n",
    "- image: images/cross_entropy_fig1.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross EntropyÎûÄ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Ìò∏Ï∂ú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numbers\n",
    "from typing import Optional, Tuple, Sequence, Union, Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumpyÎ°ú Íµ¨ÌòÑÌïòÍ∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyCrossEntropy:\n",
    "\n",
    "    @staticmethod\n",
    "    def log_softmax(ndarray: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "        c = np.amax(ndarray, axis=axis, keepdims=True)\n",
    "        s = ndarray - c\n",
    "        nominator = np.exp(s)\n",
    "        denominator = nominator.sum(axis=axis, keepdims=True)\n",
    "        probs = nominator / denominator\n",
    "        return np.log(probs)\n",
    "\n",
    "    @staticmethod\n",
    "    def negative_log_likelihood(\n",
    "        y_pred: np.ndarray, \n",
    "        y: np.ndarray,\n",
    "        axis: int = -1,\n",
    "        reduce: str = \"mean\",\n",
    "    ) -> np.ndarray:\n",
    "        assert y_pred.ndim == 2 and y.ndim == 1\n",
    "        axis_x = np.arange(y_pred.shape[0]) if axis != 0 else y\n",
    "        axis_y = y if axis != 0 else np.arange(y_pred.shape[0])\n",
    "        log_likelihood = y_pred[axis_x, axis_y]\n",
    "        nll = -log_likelihood\n",
    "        if reduce == \"mean\":\n",
    "            return np.mean(nll)\n",
    "        elif reduce == \"sum\":\n",
    "            return np.sum(nll)\n",
    "        return nll\n",
    "\n",
    "    def cross_entropy(\n",
    "        self, \n",
    "        y_pred: np.ndarray, \n",
    "        y: np.ndarray,\n",
    "        axis: int = -1,\n",
    "        reduce: str = \"mean\",\n",
    "    ) -> np.ndarray:\n",
    "        assert axis in [0, 1, -1]\n",
    "        assert reduce in [\"mean\", \"sum\", \"none\"]\n",
    "        log_probs = self.log_softmax(y_pred)\n",
    "        ce_loss = self.negative_log_likelihood(log_probs, y, axis, reduce)\n",
    "        return ce_loss\n",
    "    \n",
    "\n",
    "nce = NumpyCrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorchÎ°ú Íµ¨ÌòÑÌïòÍ∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogSoftmax(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx: Any, tensor: Any, dim: int = -1) -> Any:\n",
    "        # softmax(x) = softmax(x+c)\n",
    "        c = torch.amax(tensor, dim=dim, keepdims=True)\n",
    "        s = tensor - c\n",
    "        # Calculate softmax\n",
    "        nominator = torch.exp(s)\n",
    "        denominator = nominator.sum(dim=dim, keepdims=True)\n",
    "        probs = nominator / denominator\n",
    "        # Calculate log\n",
    "        log_probs = torch.log(probs)\n",
    "        ctx.save_for_backward(probs, torch.tensor(dim))\n",
    "        return log_probs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, grad_outputs: Any) -> Any:\n",
    "        # https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/SoftMax.cpp#L219\n",
    "        probs, dim, = ctx.saved_tensors\n",
    "        grad_outputs -= probs * grad_outputs.sum(dim=dim.item(), keepdims=True)\n",
    "        return grad_outputs, None\n",
    "\n",
    "\n",
    "class NegativeLogLikelihoodLoss(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx: Any, y_pred: Any, y: Any, dim: int = -1, \n",
    "                reduce: str = \"mean\", ignore_index: int = -1) -> Any:\n",
    "        bsz, n_classes = torch.tensor(y_pred.size())\n",
    "        mask = y.ne(ignore_index)\n",
    "        ctx.save_for_backward(\n",
    "            bsz, n_classes, y, \n",
    "            torch.tensor(dim), mask,\n",
    "            torch.tensor({\"mean\": 0, \"sum\": 1, \"none\": 2}.get(reduce, -1)),\n",
    "            torch.tensor(ignore_index)\n",
    "        )\n",
    "        dim_x = torch.arange(bsz) if dim != 0 else y\n",
    "        dim_y = y if dim != 0 else torch.arange(bsz)\n",
    "        log_likelihood = y_pred[dim_x, dim_y] # Calculate Log Likelihood\n",
    "        nll = -log_likelihood # Calculate Negative Log Likelihood\n",
    "        # Calculate Loss\n",
    "        if reduce == \"mean\":\n",
    "            return torch.mean(nll[mask])\n",
    "        elif reduce == \"sum\":\n",
    "            return torch.sum(nll[mask])\n",
    "        nll[~mask] = 0.\n",
    "        return nll\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, grad_outputs: Any) -> Any:\n",
    "        bsz, n_classes, y, dim, mask, reduce, ignore_index, = ctx.saved_tensors\n",
    "        if reduce.item() != 2: # reduce case\n",
    "            grad_outputs = grad_outputs.expand(bsz)\n",
    "        if reduce.item() == 0: # mean case\n",
    "            grad_outputs = grad_outputs / mask.sum()\n",
    "        negative_mean_grad = -grad_outputs # backward negative\n",
    "        # backward log likelihood (indexing)\n",
    "        if dim.item() != 0:\n",
    "            ll_grad = torch.zeros(bsz, n_classes, device=grad_outputs.device)\n",
    "            ll_grad[torch.arange(bsz), y] = 1\n",
    "            ll_grad[torch.arange(bsz), ignore_index.item()] = 0\n",
    "        else:\n",
    "            ll_grad = torch.zeros(n_classes, bsz, device=grad_outputs.device)\n",
    "            ll_grad[y, torch.arange(bsz)] = 1\n",
    "            ll_grad[ignore_index.item(), torch.arange(bsz)] = 0\n",
    "        grad_outputs = torch.diag(negative_mean_grad) @ ll_grad\n",
    "        return grad_outputs, None, None, None, None\n",
    "    \n",
    "\n",
    "_TensorOrTensors = Union[torch.Tensor, Sequence[torch.Tensor]]\n",
    "\n",
    "\n",
    "class CrossEntropyLoss(nn.Module):\n",
    "    \n",
    "    log_softmax = LogSoftmax.apply\n",
    "    negative_log_likelihood = NegativeLogLikelihoodLoss.apply\n",
    "    \n",
    "    def __init__(self, reduce: str = \"mean\", ignore_index: int = -1):\n",
    "        self.reduce = reduce\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        y_pred: _TensorOrTensors, \n",
    "        y: _TensorOrTensors,\n",
    "        dim: int = -1,\n",
    "    ) -> _TensorOrTensors:\n",
    "        log_probs = self.log_softmax(y_pred, dim)\n",
    "        ce_loss = self.negative_log_likelihood(\n",
    "            log_probs, y, dim, self.reduce, self.ignore_index)\n",
    "        probs = torch.exp(log_probs)\n",
    "        self.save_for_backward(probs, y, y_pred.size(0), y_pred.size(-1))\n",
    "        return ce_loss\n",
    "\n",
    "    def save_for_backward(self, *args):\n",
    "        self.saved_tensors = args\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def backward(self, grad_outputs: _TensorOrTensors) -> _TensorOrTensors:\n",
    "        probs, y, bsz, num_classes, = self.saved_tensors\n",
    "        y = torch.nn.functional.one_hot(y, num_classes=num_classes)\n",
    "        ce_grad = probs - y\n",
    "        if self.reduce == \"mean\":\n",
    "            ce_grad = ce_grad / bsz\n",
    "        return grad_outputs * ce_grad\n",
    "    \n",
    "    \n",
    "class PyTorchCrossEntropy:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.log_softmax = LogSoftmax.apply\n",
    "        self.negative_log_likelihood = NegativeLogLikelihoodLoss.apply\n",
    "        self.cross_entropy = CrossEntropyLoss()\n",
    "        \n",
    "\n",
    "tce = PyTorchCrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure Í∑∏Î¶¨Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Í≤∞Í≥ºÍ∞í ÎπÑÍµê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "vocab_size = 3000\n",
    "\n",
    "rtol = 1e-4\n",
    "atol = 1e-6\n",
    "isclose = partial(torch.isclose, rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [[random.normalvariate(mu=0., sigma=1.) for _ in range(vocab_size)] for _ in range(batch_size)]\n",
    "y_pred_torch = torch.FloatTensor(y_pred)\n",
    "y_pred_torch.requires_grad = True\n",
    "y_pred_numpy = y_pred_torch.detach().numpy()\n",
    "\n",
    "y = [random.randint(0, vocab_size) for _ in range(batch_size)]\n",
    "y_torch = torch.LongTensor(y)\n",
    "y_numpy = y_torch.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.4246, 8.8899, 9.0440, 8.9059, 6.7580, 7.8764, 7.6204, 7.6686],\n",
       "       grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.nll_loss(\n",
    "    nn.functional.log_softmax(y_pred_torch, dim=-1), \n",
    "    y_torch, \n",
    "    reduction=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 3 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-272-fb0b9a06ac09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0my_torch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;34m\"sum\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() takes 3 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "tce.negative_log_likelihood(\n",
    "    nn.functional.log_softmax(y_pred_torch, dim=-1), \n",
    "    y_torch, \n",
    "    -1,\n",
    "    \"sum\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.205082 , 6.5644517, 7.9004273, 9.04045  , 7.5050826, 8.987215 ,\n",
       "       7.9438257, 7.5462923], dtype=float32)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nce.cross_entropy(y_pred_numpy, y_numpy, reduce=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do both output the same tensors? üî•\n"
     ]
    }
   ],
   "source": [
    "ce_result = nn.CrossEntropyLoss()(y_pred_torch, y_torch)\n",
    "ce_numpy = nce.cross_entropy(y_pred_numpy, y_numpy)\n",
    "ce_torch = tce.cross_entropy(y_pred_torch, y_torch)\n",
    "\n",
    "try:\n",
    "    isclose(ce_result, ce_torch).item()\n",
    "    isclose(ce_result, torch.tensor(ce_numpy)).item()\n",
    "    success = True\n",
    "except:\n",
    "    success = False\n",
    "\n",
    "print(\"Do both output the same tensors?\", \"üî•\" if success else \"üí©\")\n",
    "if not success:\n",
    "    raise Exeption(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do both output the same tensors? üî•\n"
     ]
    }
   ],
   "source": [
    "# backward (under debugging)\n",
    "ce_grad = torch.autograd.grad(ce_result, y_pred_torch, retain_graph=True)[0]\n",
    "my_ce_grad1 = torch.autograd.grad(ce_torch, y_pred_torch, retain_graph=True)[0]\n",
    "my_ce_grad2 = tce.cross_entropy.backward(torch.ones_like(y_pred_torch))\n",
    "# my_ce_grad2 = tce.cross_entropy.backward(y_pred_torch)\n",
    "\n",
    "try:\n",
    "    isclose(ce_grad, my_ce_grad1).all()\n",
    "    isclose(ce_grad, my_ce_grad2).all()\n",
    "    success = True\n",
    "except:\n",
    "    success = False\n",
    "\n",
    "print(\"Do both output the same tensors?\", \"üî•\" if success else \"üí©\")\n",
    "if not success:\n",
    "    raise Exeption(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- reduce Í∏∞Îä• Ï∂îÍ∞Ä\n",
    "- forwardÏôÄ backward ÏàúÏÑúÎåÄÎ°ú ÏàòÏãùÏúºÎ°ú ÏÑ§Î™ÖÌïòÎ©¥ÏÑú ÏΩîÎìúÎ°ú Ïó∞Í≤∞ÏãúÌÇ§Í∏∞\n",
    "- plot Í∑∏Î¶¨Í∏∞"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
