{
  
    
        "post0": {
            "title": "Cross Entropy",
            "content": "Introduction . ë³¸ í¬ìŠ¤íŒ…ì—ì„œëŠ” Cross Entropyì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. . Cross Entropyì˜ ê°œë… | Cross Entropy ìˆ˜ì‹ | Matplotlibìœ¼ë¡œ figure ê·¸ë ¤ë³´ê¸° | Numpyë¡œ ì½”ë“œ êµ¬í˜„ì²´ ì‚´í´ë³´ê¸° | PyTorchë¡œ forward, backward pass ìˆ˜ì‹ê³¼ í•¨ê»˜ ì´í•´í•˜ê¸° | PyTorch ê²°ê³¼ê°’ê³¼ ë™ì¼í•œ ì¶œë ¥ì´ ë‚˜ì˜¤ë„ë¡ ì½”ë“œ ìˆ˜ì • | . Cross Entropy&#46976;? . Cross entropyëŠ” random variable(í™•ë¥  ë³€ìˆ˜) í˜¹ì€ set of events(ì‚¬ê±´ ì§‘í•©)ì´ ì£¼ì–´ì¡Œì„ ë•Œ ì„œë¡œ ë‹¤ë¥¸ ë‘ í™•ë¥  ë¶„í¬ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ì¬ëŠ” measureì…ë‹ˆë‹¤. . Entropy . Event $x$ê°€ ë“±ì¥í•  í™•ë¥ ì„ $P(x)$ë¼ê³  ë‘ê³  ì´ì— ëŒ€í•œ ì •ë³´ëŸ‰ $h(x)$ì— ëŒ€í•œ ìˆ˜ì‹ì€ ì•„ë˜ì™€ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤. . $$h(x)=- log{P(x)}$$ . $ log$ í•¨ìˆ˜ì˜ ê·¸ë˜í”„ì— ìŒìˆ˜ë¥¼ ì”Œìš°ë©´ ì•„ë˜ì™€ ê°™ì´ ê·¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . import numpy as np import matplotlib.pyplot as plt x = np.linspace(0.01, 1, 100) y = - np.log(x) plt.axvline(0, color=&quot;k&quot;, alpha=0.7) plt.axhline(0, color=&quot;k&quot;, alpha=0.7) plt.plot(x, y, lw=3) plt.xlabel(&quot;probability&quot;) plt.ylabel(&quot;information&quot;) plt.grid() _ = plt.plot() . ìœ„ ê·¸ë˜í”„ë¡œë¶€í„° ì•„ë˜ì˜ ì‚¬ì‹¤ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . $P(x)$ê°€ 0.0ì— ê°€ê¹Œìš¸ ë•Œ (ì‚¬ê±´ì´ ë“±ì¥í•  í™•ë¥ ì´ í¬ë°•í•  ë•Œ) í•´ë‹¹ ì‚¬ê±´ì— ëŒ€í•œ ì •ë³´ëŸ‰ì´ ë†’ìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. | $P(x)$ê°€ 1.0ì— ê°€ê¹Œìš¸ ë•Œ (ì‚¬ê±´ì´ ë“±ì¥í•  í™•ë¥ ì´ ë†’ì„ ë•Œ) í•´ë‹¹ ì‚¬ê±´ì— ëŒ€í•œ ì •ë³´ëŸ‰ì´ ë‚®ìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. | . ë•Œë¬¸ì— ì´ë¥¼ ì •ë³´ ì´ë¡ ì—ì„œëŠ” surprise ë¼ê³  ë¬˜ì‚¬í•©ë‹ˆë‹¤. ìš°ë¦¬ê°€ ë§¤ì¼ ë§ˆì£¼í•˜ëŠ” ì¼ìƒì—ì„œëŠ” í¬ê²Œ ë†€ë„ë§Œí•œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì·¨ì—…ì„ í–ˆë‹¬ì§€ ì—°ì• ë¥¼ ì‹œì‘í•œë‹¤ë˜ì§€ ì¸ìƒì— ë“œë¬¼ê²Œ ì°¾ì•„ì˜¤ëŠ” ì‚¬ê±´ì´ ìƒê¸°ë©´ ìš°ë¦¬ëŠ” ì´ ê¸°ì¨ì— ì·¨í•˜ë©° ë†€ë¼ê³¤ í•©ë‹ˆë‹¤. . ì¦‰, ë“œë¬¼ê²Œ ë°œìƒí•  ìˆ˜ë¡ ë”ìš± ì •ë³´ëŸ‰ì´ ë†’ì€ ê²ƒì´ì§€ìš”. . ë‹¨ì¼ ì‚¬ê±´ì— ëŒ€í•œ ì •ë³´ëŸ‰ë§ê³  set of $x$, $X$ë¥¼ ìƒê°í•´ë³´ê² ìŠµë‹ˆë‹¤. $X$ëŠ” ì´ì‚° í™•ë¥  ë¶„í¬ë¼ê³  ê°€ì •í• ê²Œìš”. ê·¸ëŸ¬ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì‹ì„ ì ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . $$H(X)=- sum_{x in X}P(x) cdot log{P(x)}$$ . $X$ì˜ ëª¨ë“  ì‚¬ê±´ $x$ì— ëŒ€í•œ ì •ë³´ëŸ‰ë“¤ì„ ë”í•œ ê°’ì„ $X$ì— ëŒ€í•œ ì •ë³´ëŸ‰ìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤ë¼ê³  ì‹ì´ ì í˜€ìˆìŠµë‹ˆë‹¤. EntropyëŠ” ê°€ëŠ¥í•œ ëª¨ë“  ì‚¬ê±´ì´ ë°œìƒí•  í™•ë¥ ì´ ì „ë¶€ ê°™ì„ ë•Œ ìµœëŒ“ê°’ì„ ê°€ì§‘ë‹ˆë‹¤. ê°ê°ì˜ ì •ë³´ëŸ‰ì€ ë°œìƒí•  í™•ë¥ ì´ ì‘ì„ìˆ˜ë¡ ê°’ì´ ì»¤ì§€ê¸° ë•Œë¬¸ì´ì£ . . &#51060;&#49328;&#48320;&#49688;&#51032; Entropy . Discrete variableì˜ ê¸°ëŒ“ê°’ì€ summationìœ¼ë¡œ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ì˜ ìˆ˜ì‹ì„ ì˜ ì‚´í´ë³´ì£ . ê¸°ëŒ“ê°’ì˜ ìˆ˜ì‹ì´ì£ ? . $$ begin{align} H(X)&amp;=- sum_{x in X}P(x) cdot log{P(x)} &amp;= sum_{x in X}P(x) cdot(- log{P(x)}) &amp;= mathbb{E} big[- log{P(x)} big] end{align}$$ìœ„ ìˆ˜ì‹ì„ ì˜ ê¸°ì–µí•´ì£¼ì„¸ìš”. ì´ì‚°ë³€ìˆ˜ëŠ” ê¸°ëŒ“ê°’ì„ í•©ìœ¼ë¡œ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ì´ ì‰½ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . í™•ë¥  ë¶„í¬ Yê°€ ì´ì‚°ë¶„í¬ì´ê³  ë‹¤ìŒê³¼ ê°™ë‹¤ê³  ê°€ì •í•´ë³´ê² ìŠµë‹ˆë‹¤. . í™•ë¥  ë¶„í¬ $Y_1$: $P(Y=0)=0.8, ;P(Y=1)=0.2$ | . $Y_1$ì— ëŒ€í•œ entropyë¥¼ ì •ì˜ëœ ì‹ê³¼ ê°™ì´ ë™ì¼í•˜ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . p_y = [0.8, 0.2] sum([p * -np.log2(p) for p in p_y]) . 0.7219280948873623 . &#50672;&#49549;&#48320;&#49688;&#51032; Entropy . ì´ì™€ëŠ” ë‹¤ë¥´ê²Œ Continuous variableì˜ ê¸°ëŒ“ê°’ì€ Integralë¡œ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . $$ begin{align} H(X)&amp;= mathbb{E} big[- log{P(X)} big] &amp;= int_{- infty}^{ infty}{P(X) cdot(- log{P(X)})}dx end{align}$$ìœ„ë¥¼ trapezoidal ruleì„ ì‚¬ìš©í•˜ì—¬ ì ë¶„ê°’ì„ ê³„ì‚°í•´ë³´ê² ìŠµë‹ˆë‹¤. . def normal(x, mu, sigma): var = sigma ** 2 x = x - mu return (1 / np.sqrt(2 * np.pi * var)) * np.exp(-x**2 / (2 * var)) entropy = lambda p: -p * np.log(p) def trapezoidal_rule(dt, p, f): return np.sum((f(p[:-1]) + f(p[1:])) * dt) / 2 xlim, ylim, n_sample, n_bin = 10, 0.5, 50000, 1000 yticks = [0.1, 0.2, 0.3, 0.4, 0.5] mu1, mu2 = 0.0, 0.0 sigma1, sigma2 = 1.0, 2.5 x = np.linspace(-xlim, xlim, n_sample) X1 = np.random.normal(loc=mu1, scale=sigma1, size=(n_sample)) X2 = np.random.normal(loc=mu2, scale=sigma2, size=(n_sample)) fig = plt.figure(figsize=(8, 4)) def plot_normal_entropy(ax, X, mu, sigma): _, bins, _ = ax.hist(X, bins=n_bin, density=True, histtype=&quot;stepfilled&quot;, color=&quot;slateblue&quot;, edgecolor=&quot;k&quot;, lw=0.1) ax.plot(x, normal(x, mu, sigma), color=&quot;k&quot;, lw=2) ax.set_ylim(0, ylim) ax.set_yticks(yticks) dt = np.diff(bins) p = normal(bins, mu, sigma) ent = trapezoidal_rule(dt, p, entropy) ax.set_title(rf&quot;N(${mu}$, ${sigma}^2$) H(p)={ent:.4f}&quot;) plot_normal_entropy(fig.add_subplot(1, 2, 1), X1, mu1, sigma1) plt.ylabel(&quot;probability&quot;) plot_normal_entropy(fig.add_subplot(1, 2, 2), X2, mu2, sigma2) _ = plt.plot() . ìœ„ì²˜ëŸ¼ ì§ì ‘ ì ë¶„ê°’ì„ ê·¼ì‚¬í•˜ì—¬ ê³„ì‚°í•  ìˆ˜ë„ ìˆì§€ë§Œ ì •ê·œ ë¶„í¬ ì—”íŠ¸ë¡œí”¼ ê°’ì„ í•´ì„ì ìœ¼ë¡œë„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . ì •ê·œ ë¶„í¬ì˜ ìˆ˜ì‹ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. . $$P(X)= cfrac{1}{ sqrt{2 pi sigma^2}} exp{ bigg( cfrac{-(X- mu)^2}{2 sigma^2} bigg)}$$ . ì´ì œ ì—”íŠ¸ë¡œí”¼ ìˆ˜ì‹ì„ ì „ê°œí•´ë´…ì‹œë‹¤. . $$ begin{align} H(X)&amp;= mathbb{E} big[- log{P(X)} big] &amp;= int_{- infty}^{ infty}{P(X) cdot(- log{P(X)})}dX end{align}$$ì—¬ê¸°ì„œ $- log P(X)$ëŠ” $ cfrac{1}{2} log{2 pi sigma^2}+ cfrac{1}{2 sigma^2}(X- mu)^2$ì´ê³  ì´ë¥¼ ëŒ€ì…í•˜ë©´ . $$H(X)= cfrac{1}{2} log{2 pi sigma^2} int_{- infty}^{ infty}{P(X)}dX+ cfrac{1} {2 sigma^2} int_{- infty}^{ infty}(X- mu)^2P(X)dX$$ì—¬ê¸°ì„œ í™•ë¥  ë° ë¶„ì‚°ì˜ ì •ì˜ì— ë”°ë¼ ë‹¤ìŒì„ ì•Œ ìˆ˜ ìˆê³  . $$ int_{- infty}^{ infty}{P(X)}dX=1$$ $$ int_{- infty}^{ infty}(X- mu)^2P(X)dX= sigma^2$$ . ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹ì„ ë‹¤ì‹œ ì‘ì„±í•˜ë©´ . $$H(X)= cfrac{1}{2} big( log{2 pi sigma^2}+1 big)$$ . ì™€ ê°™ì´ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . python codeë¡œ êµ¬í˜„í•˜ì—¬ ìœ„ì˜ ê²°ê³¼ì™€ ë¹„êµí•´ë´…ì‹œë‹¤. . def normal_entropy(sigma): # return 0.5 * np.log(np.e * 2 * np.pi * sigma**2) return 0.5 * (1 + np.log(2 * np.pi * sigma**2)) normal_entropy(sigma1) . 1.4189385332046727 . Cross Entropy . Machine Learning: A Probabilistic Perspectiveì—ì„œëŠ” cross entropyë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ëª…í•©ë‹ˆë‹¤. . The cross entropy is the average number of bits needed to encode data coming from a source with distribution p when we use model q ... . $P$ëŠ” ëª¨ë¸ë§í•˜ê³ ì í•˜ëŠ” ë¶„í¬, $Q$ëŠ” ëª¨ë¸ë§ì„ í•  ë¶„í¬ë¼ê³  ìƒê°í•˜ë©´ ì´í•´ê°€ ë¹ ë¦…ë‹ˆë‹¤. $P$ ëŒ€ì‹  $Q$ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ê±´ì„ ë‚˜íƒ€ë‚´ëŠ” ì´ ë¹„íŠ¸ì˜ í‰ê·  ìˆ˜ ì…ë‹ˆë‹¤. . $$H(P,Q)=- sum_{x in X}P(x) cdot log Q(x)$$ . ì´ ë¹„íŠ¸ ìˆ˜ê°€ ì•„ë‹ˆë¼ ì¶”ê°€ë¡œ í•„ìš”í•œ ë¹„íŠ¸ì˜ í‰ê· ê°’ì€? relative entropy, Kullback-Leibler Divergence ë¼ê³  í•©ë‹ˆë‹¤. . $$KL(P||Q)=- sum_{x in X}P(x) cdot log{ cfrac{Q(x)}{P(x)}}$$ . ìœ„ ìˆ˜ì‹ì€ Entropyì™€ ì—®ì–´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . $$ begin{align} H(P)&amp;= sum_{x in X} P(x) cdot log P(x) &amp;=H(P,Q)-KL(P||Q) end{align}$$ Cross-entropy and Maximum Likelihood Estimation . ë¶„ë¥˜ ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ Neural Networkë¥¼ í•™ìŠµì‹œí‚¬ ë•Œ, ìš°ë¦¬ëŠ” í”íˆ Cross Entropyë¡œ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ì™œì¼ê¹Œìš”? . ìœ„ì—ì„œ Entropy, Cross Entropy, KL-Divergenceì— ëŒ€í•œ ìˆ˜ì‹ì„ ì •ì˜í–ˆìŠµë‹ˆë‹¤. . ì•ì„œ í™•ë¥  ë³€ìˆ˜ì˜ Entropy ì •ì˜ì—ì„œ Entropyê°€ í™•ë¥  ë³€ìˆ˜ì˜ Expectationê³¼ ê´€ë ¨ì´ ìˆìŒì„ í™•ì¸í–ˆì—ˆìŠµë‹ˆë‹¤. ê°ê°ì„ ê¸°ëŒ“ê°’ í‘œí˜„ìœ¼ë¡œ ì¨ë³´ë©´, . $$ begin{align} H(X)&amp;=- sum_x p(x) log p(x) &amp;= sum_x -p(x) log p(x) &amp;= sum_x p(x) log cfrac{1}{p(x)} &amp;= mathbb{E}_{X sim p(x)} bigg[ log cfrac{1}{p(x)} bigg] end{align}$$$$ begin{align} KL(p||q)&amp;= sum_x p(x) log cfrac{p(x)}{q(x)} &amp;= mathbb{E}_{X sim p(x)} bigg[ log cfrac{p(x)}{q(x)} bigg] &amp;= mathbb{E}_{X sim p(x)} bigg[ log cfrac{1}{q(x)}- cfrac{1}{p(x)} bigg] &amp;= mathbb{E}_{X sim p(x)} bigg[ log cfrac{1}{q(x)} bigg]- mathbb{E}_{X sim p(x)} bigg[ log cfrac{1}{p(x)} bigg] &amp;= mathbb{E}_{X sim p(x)} bigg[ log cfrac{1}{q(x)} bigg] - H(p) end{align}$$Cross entropyëŠ” ì•„ë˜ì™€ ê°™ì´ ì ì„ ìˆ˜ ìˆì£ . . $$H(p,q)= mathbb{E}_{X sim p(x)} bigg[ log cfrac{1}{q(x)} bigg]$$ . ê·¸ëŸ¬ë©´ Maximum Likelihood Estimationì—ì„œì˜ objective functionì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. . ìš°ë¦¬ëŠ” ë¶„í¬ $p$ë¥¼ ëª¨ë¸ë§í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. ì´ì— ëŒ€í•œ ë°ì´í„° $X$ë¥¼ ëª¨ìˆ˜ $ theta$ë¥¼ ê°€ì§€ëŠ” parametric model $q$ë¡œ ëª¨ë¸ë§í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ìˆ˜ì‹ì„ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . $$ theta_{ML}= underset{ theta}{ mathrm{argmax}} ;q(X; theta)$$ . i.i.d ê°€ì •ì— ì˜í•´ ì•„ë˜ì™€ ê°™ì´ ìˆ˜ì‹ì„ ì‘ì„±í•˜ë©´ (ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ) . $$ begin{align} theta_{ML}&amp;= underset{ theta}{ mathrm{argmax}} ; prod_{i=1}^{m}q(x_i; theta) &amp;= underset{ theta}{ mathrm{argmax}} ; sum_{i=1}^{m} log q(x_i; theta) &amp;= underset{ theta}{ mathrm{argmax}} ; sum_{i=1}^{m} frac{1}{m} log q(x_i; theta) &amp;= underset{ theta}{ mathrm{argmax}} ; mathbb{E}_{X sim p(x)} big[ log q(x) big] &amp;= underset{ theta}{ mathrm{argmin}} ;- mathbb{E}_{X sim p(x)} big[ log q(x) big] &amp;= underset{ theta}{ mathrm{argmin}} ; mathbb{E}_{X sim p(x)} big[ log frac{1}{q(x)} big] end{align}$$ì¦‰, Likelihoodë¥¼ maximizeí•˜ëŠ” ë¬¸ì œëŠ” Cross Entropyë¥¼ Minimizeí•˜ëŠ” ë¬¸ì œë¡œ ì¹˜í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . Code Implementation . Cross Entropyë¥¼ êµ¬í˜„í•´ë´…ì‹œë‹¤. . Library &#54840;&#52636; . import math import numbers from typing import Optional, Tuple, Sequence, Union, Any import numpy as np import torch import torch.nn as nn _TensorOrTensors = Union[torch.Tensor, Sequence[torch.Tensor]] . Numpy&#47196; &#44396;&#54788;&#54616;&#44592;: forward . Cross EntropyëŠ” LogSoftmaxì™€ NegativeLogLikelihoodë¡œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ì—ì„œ ë³¸ ê²ƒì²˜ëŸ¼ ìš°ë„ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë¬¸ì œì™€ Cross entropyë¥¼ ìµœì†Œí™”í•˜ëŠ” ë¬¸ì œëŠ” ë™ì¼í•˜ê¸° ë•Œë¬¸ì— ì´ë¥¼ Negative Log likelihoodë¥¼ ìµœì†Œí™”í•˜ëŠ” ë¬¸ì œë¡œ ìƒê°í•´ë„ ë¬´ë°©í•˜ê² ì£ ? ì´ë¥¼ í™œìš©í•˜ì—¬ Cross entropyë¥¼ êµ¬í•´ë´…ì‹œë‹¤. . í•œ ê°€ì§€, êµ¬í˜„ í…Œí¬ë‹‰ì„ ì†Œê°œë“œë¦¬ê³ ì í•©ë‹ˆë‹¤. softmax í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. . $$ mathrm{softmax}(x)= mathrm{softmax}(x+c)$$ . ì´ë¥¼ í™œìš©í•˜ì—¬ overflow ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . $$ mathrm{softmax}(x_i)= cfrac{exp(x_i)}{ sum_j exp(x_j)}$$ . def log_softmax_numpy(arr): c = np.amax(arr, axis=-1, keepdims=True) s = arr - c nominator = np.exp(s) denominator = nominator.sum(axis=-1, keepdims=True) probs = nominator / denominator return np.log(probs) . ìš°ë„ëŠ” fancy indexingìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. êµ¬í•œ likelihoodì— ìŒìˆ˜ë¥¼ ì”Œì›Œì£¼ë©´ Negative Likelihoodê°€ ë˜ê² ì§€ìš”. . êµ¬í•œ Negative Log LikelihoodëŠ” í‰ê· ê°’ìœ¼ë¡œ reduceí•˜ê² ìŠµë‹ˆë‹¤. . def negative_log_likelihood_numpy(y_pred, y): log_likelihood = y_pred[np.arange(y_pred.shape[0]), y] nll = -log_likelihood return nll.mean() . Cross entropyëŠ” ì´ì œ ê°„ë‹¨í•©ë‹ˆë‹¤. ì˜ˆì¸¡ ê°’ Qì— log softmaxë¥¼ ì·¨í•´ì£¼ê³  ìŒì˜ ë¡œê·¸ ê°€ëŠ¥ë„ë¥¼ ê³„ì‚°í•´ì£¼ë©´ ë©ë‹ˆë‹¤. . def cross_entropy_numpy(y_pred, y): log_probs = log_softmax_numpy(y_pred) ce_loss = negative_log_likelihood_numpy(log_probs, y) return ce_loss . PyTorch&#47196; &#44396;&#54788;&#54616;&#44592;: forward . ì´ë¥¼ pytorchë¡œë„ êµ¬í˜„í•´ë´…ì‹œë‹¤. êµ¬í˜„ì€ ë©”ì„œë“œëª…ê¹Œì§€ ë™ì¼í•©ë‹ˆë‹¤. ì°¨ì´ë¼ë©´ numpyì—ì„œëŠ” ì°¨ì›ì¶•ì„ axisë¼ëŠ” parameterë¡œ ë°›ëŠ” ë°˜ë©´, torchëŠ” dimì´ë¼ëŠ” parameterë¡œ ë°›ìŠµë‹ˆë‹¤. . def log_softmax_torch(tensor): c = torch.amax(tensor, dim=-1, keepdims=True) s = tensor - c nominator = torch.exp(s) denominator = nominator.sum(dim=-1, keepdims=True) probs = nominator / denominator return torch.log(probs) def negative_log_likelihood_torch(y_pred, y): log_likelihood = y_pred[torch.arange(y_pred.shape[0]), y] nll = -log_likelihood return nll.mean() def cross_entropy_torch(y_pred, y): log_probs = log_softmax_torch(y_pred) ce_loss = negative_log_likelihood_torch(log_probs, y) return ce_loss . forward &#44208;&#44284;&#44050; &#48708;&#44368; . import random from functools import partial batch_size = 8 vocab_size = 3000 rtol = 1e-4 atol = 1e-6 isclose = partial(torch.isclose, rtol=rtol, atol=atol) . y_pred = [ [random.normalvariate(mu=0., sigma=1.) for _ in range(vocab_size)] for _ in range(batch_size) ] y_pred_torch = torch.FloatTensor(y_pred) y_pred_torch.requires_grad = True y_pred_numpy = y_pred_torch.detach().numpy() y = [random.randint(0, vocab_size) for _ in range(batch_size)] y_torch = torch.LongTensor(y) y_numpy = y_torch.numpy() . ce_result = nn.CrossEntropyLoss()(y_pred_torch, y_torch) ce_numpy = cross_entropy_numpy(y_pred_numpy, y_numpy) ce_torch = cross_entropy_torch(y_pred_torch, y_torch) try: isclose(ce_result, ce_torch).item() isclose(ce_result, torch.tensor(ce_numpy)).item() success = True except: success = False print(&quot;Do both output the same tensors?&quot;, &quot;ğŸ”¥&quot; if success else &quot;ğŸ’©&quot;) if not success: raise Exeption(&quot;Something went wrong&quot;) . Do both output the same tensors? ğŸ”¥ . PyTorch&#47196; &#44396;&#54788;&#54616;&#44592;: backward . Log Softmax . $$o(1-o)$$ . 1ì€ í¬ë¡œë„¤í´ ë¸íƒ€ | log softmaxê°€ ê³„ì‚° ìƒ ì´ì ì´ í¼ | lossë¥¼ ë” í¬ê²Œ ë§Œë“¤ì–´ ì£¼ê¸°ë„ | softmaxì˜ backwardì˜ grad_outputsì— log í•¨ìˆ˜ì˜ ì—­í•¨ìˆ˜ì¸ 1/xë¥¼ ë„£ì–´ì£¼ë©´ log_softmaxì˜ backward formì´ ë‚˜ì˜¨ë‹¤. | . def _softmax(tensor: torch.Tensor, dim: int = -1) -&gt; torch.Tensor: c = torch.amax(tensor, dim=dim, keepdims=True) s = tensor - c nominator = torch.exp(s) denominator = nominator.sum(dim=dim, keepdims=True) probs = nominator / denominator return probs class Softmax(torch.autograd.Function): @staticmethod def forward(ctx: Any, tensor: Any, dim: int = -1) -&gt; Any: probs = _softmax(tensor) ctx.save_for_backward(probs, torch.tensor(dim)) return probs @staticmethod def backward(ctx: Any, grad_outputs: Any) -&gt; Any: probs, dim, = ctx.saved_tensors grad_outputs -= (grad_outputs * probs).sum(dim.item(), keepdims=True) return probs * grad_outputs, None softmax = Softmax.apply class LogSoftmax(torch.autograd.Function): @staticmethod def forward(ctx: Any, tensor: Any, dim: int = -1) -&gt; Any: probs = _softmax(tensor) ctx.save_for_backward(probs, torch.tensor(dim)) return torch.log(probs) @staticmethod def backward(ctx: Any, grad_outputs: Any) -&gt; Any: probs, dim, = ctx.saved_tensors grad_outputs -= probs * grad_outputs.sum(dim=dim.item(), keepdims=True) return grad_outputs, None log_softmax = LogSoftmax.apply . x = torch.randn(5, 3, requires_grad=True) # Softmax check # -- forward pass y_orig = nn.functional.softmax(x, dim=-1) y_impl = y = softmax(x, -1) assert isclose(y_orig, y_impl).all(), &quot;ğŸ’©&quot; # -- backward pass dy_orig = torch.autograd.grad(y_orig, x, torch.ones_like(x), retain_graph=True)[0] dy_impl = torch.autograd.grad(y_impl, x, torch.ones_like(x), retain_graph=True)[0] assert isclose(dy_orig, dy_impl).all(), &quot;ğŸ’©&quot; # LogSoftmax check # -- forward pass y_orig = nn.functional.log_softmax(x, dim=-1) y_impl = y = log_softmax(x, -1) assert isclose(y_orig, y_impl).all(), &quot;ğŸ’©&quot; # -- backward pass dy_orig = torch.autograd.grad(y_orig, x, torch.ones_like(x), retain_graph=True)[0] dy_impl = torch.autograd.grad(y_impl, x, torch.ones_like(x), retain_graph=True)[0] assert isclose(dy_orig, dy_impl).all(), &quot;ğŸ’©&quot; # Log + Softmax check # -- forward pass y1 = torch.log(softmax(x, -1)) y2 = log_softmax(x, -1) assert isclose(y1, y2).all(), &quot;ğŸ’©&quot; # -- backward pass dy1 = torch.autograd.grad(y1, x, torch.ones_like(x), retain_graph=True)[0] dy2 = torch.autograd.grad(y2, x, torch.ones_like(x), retain_graph=True)[0] assert isclose(dy1, dy2).all(), &quot;ğŸ’©&quot; print(&quot;ğŸ”¥&quot;) . ğŸ”¥ . Negative Log Likelihood . class NegativeLogLikelihoodLoss(torch.autograd.Function): @staticmethod def forward(ctx: Any, y_pred: Any, y: Any) -&gt; Any: bsz, n_classes = torch.tensor(y_pred.size()) ctx.save_for_backward(bsz, n_classes, y) log_likelihood = y_pred[torch.arange(bsz), y] nll = -log_likelihood return nll.mean() @staticmethod def backward(ctx: Any, grad_outputs: Any) -&gt; Any: bsz, n_classes, y, = ctx.saved_tensors grad_outputs = grad_outputs.expand(bsz) / bsz negative_grad = -grad_outputs ll_grad = torch.zeros(bsz, n_classes, device=grad_outputs.device) ll_grad[torch.arange(bsz), y] = 1. grad_outputs = torch.diag(negative_grad) @ ll_grad return grad_outputs, None nll_loss = NegativeLogLikelihoodLoss.apply . Cross Entropy . class CrossEntropyLoss(nn.Module): def forward( self, y_pred: _TensorOrTensors, y: _TensorOrTensors ) -&gt; _TensorOrTensors: log_probs = log_softmax(y_pred) ce_loss = nll_loss(log_probs, y) probs = torch.exp(log_probs) / log_probs.size(0) self.save_for_backward(probs, y, y_pred.size(-1)) return ce_loss def save_for_backward(self, *args): self.saved_tensors = args @torch.no_grad() def backward(self, grad_outputs: _TensorOrTensors) -&gt; _TensorOrTensors: probs, y, num_classes, = self.saved_tensors ce_grad = probs - torch.nn.functional.one_hot(y, num_classes=num_classes) return grad_outputs * ce_grad . put everything together . ìœ„ì˜ ì„¸ ëª¨ë“ˆì„ í•œë° ëª¨ì•„ êµ¬í˜„ | ignore_index, reduction ì¶”ê°€ êµ¬í˜„ | ì„¤ëª…ì€ ì‹œê°„ì´ ë˜ë©´ ì¶”ê°€ë¡œ í¬ìŠ¤íŒ… ì—…ë°ì´íŠ¸ | . class LogSoftmax(torch.autograd.Function): @staticmethod def forward(ctx: Any, tensor: Any, dim: int = -1) -&gt; Any: # softmax(x) = softmax(x+c) c = torch.amax(tensor, dim=dim, keepdims=True) s = tensor - c # Calculate softmax nominator = torch.exp(s) denominator = nominator.sum(dim=dim, keepdims=True) probs = nominator / denominator # Calculate log log_probs = torch.log(probs) ctx.save_for_backward(probs, torch.tensor(dim)) return log_probs @staticmethod def backward(ctx: Any, grad_outputs: Any) -&gt; Any: # https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/SoftMax.cpp#L219 probs, dim, = ctx.saved_tensors grad_outputs -= probs * grad_outputs.sum(dim=dim.item(), keepdims=True) return grad_outputs, None class NegativeLogLikelihoodLoss(torch.autograd.Function): @staticmethod def forward(ctx: Any, y_pred: Any, y: Any, dim: int = -1, reduce: str = &quot;mean&quot;, ignore_index: int = -1) -&gt; Any: bsz, n_classes = torch.tensor(y_pred.size()) mask = y.ne(ignore_index) ctx.save_for_backward( bsz, n_classes, y, torch.tensor(dim), mask, torch.tensor({&quot;mean&quot;: 0, &quot;sum&quot;: 1, &quot;none&quot;: 2}.get(reduce, -1)), torch.tensor(ignore_index) ) dim_x = torch.arange(bsz) if dim != 0 else y dim_y = y if dim != 0 else torch.arange(bsz) log_likelihood = y_pred[dim_x, dim_y] # Calculate Log Likelihood nll = -log_likelihood # Calculate Negative Log Likelihood # Calculate Loss if reduce == &quot;mean&quot;: return torch.mean(nll[mask]) elif reduce == &quot;sum&quot;: return torch.sum(nll[mask]) nll[~mask] = 0. return nll @staticmethod def backward(ctx: Any, grad_outputs: Any) -&gt; Any: bsz, n_classes, y, dim, mask, reduce, ignore_index, = ctx.saved_tensors if reduce.item() != 2: # reduce case grad_outputs = grad_outputs.expand(bsz) if reduce.item() == 0: # mean case grad_outputs = grad_outputs / mask.sum() negative_mean_grad = -grad_outputs # backward negative # backward log likelihood (indexing) if dim.item() != 0: ll_grad = torch.zeros(bsz, n_classes, device=grad_outputs.device) ll_grad[torch.arange(bsz), y] = 1 ll_grad[torch.arange(bsz), ignore_index.item()] = 0 else: ll_grad = torch.zeros(n_classes, bsz, device=grad_outputs.device) ll_grad[y, torch.arange(bsz)] = 1 ll_grad[ignore_index.item(), torch.arange(bsz)] = 0 grad_outputs = torch.diag(negative_mean_grad) @ ll_grad return grad_outputs, None, None, None, None class CrossEntropyLoss(nn.Module): log_softmax = LogSoftmax.apply negative_log_likelihood = NegativeLogLikelihoodLoss.apply def __init__(self, reduce: str = &quot;mean&quot;, ignore_index: int = -1): super().__init__() self.reduce = reduce self.ignore_index = ignore_index def forward( self, y_pred: _TensorOrTensors, y: _TensorOrTensors, dim: int = -1, ) -&gt; _TensorOrTensors: log_probs = self.log_softmax(y_pred, dim) ce_loss = self.negative_log_likelihood( log_probs, y, dim, self.reduce, self.ignore_index) probs = torch.exp(log_probs) self.save_for_backward(probs, y, y_pred.size(0), y_pred.size(-1)) return ce_loss def save_for_backward(self, *args): self.saved_tensors = args @torch.no_grad() def backward(self, grad_outputs: _TensorOrTensors) -&gt; _TensorOrTensors: probs, y, bsz, num_classes, = self.saved_tensors y = torch.nn.functional.one_hot(y, num_classes=num_classes) ce_grad = probs - y if self.reduce == &quot;mean&quot;: ce_grad = ce_grad / bsz return grad_outputs * ce_grad . Reference . https://machinelearningmastery.com/cross-entropy-for-machine-learning/ | https://stackoverflow.com/questions/61567597/how-is-log-softmax-implemented-to-compute-its-value-and-gradient-with-better | https://math.stackexchange.com/questions/1804805/how-is-the-entropy-of-the-normal-distribution-derived | https://datascienceschool.net/02%20mathematics/10.01%20%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC.html | https://medium.com/konvergen/cross-entropy-and-maximum-likelihood-estimation-58942b52517a | https://discuss.pytorch.org/t/logsoftmax-vs-softmax/21386/4 | https://stackoverflow.com/questions/61567597/how-is-log-softmax-implemented-to-compute-its-value-and-gradient-with-better | https://github.com/pytorch/pytorch/issues/31829 | https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/SoftMax.cpp#L219 | https://github.com/jinmang2/boostcamp_ai_tech_2/blob/main/u-stage/nlp/ch03_rnn/implement_rnn.py | .",
            "url": "https://jinmang2.github.io/implementation/ai-math/2022/02/07/cross-entropy.html",
            "relUrl": "/implementation/ai-math/2022/02/07/cross-entropy.html",
            "date": " â€¢ Feb 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.â†© . 2. This is the other footnote. You can even have a link!â†© .",
            "url": "https://jinmang2.github.io/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " â€¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a â€œlevel 1 headingâ€ in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Hereâ€™s a footnote 1. Hereâ€™s a horizontal rule: . . Lists . Hereâ€™s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes â€¦andâ€¦ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.Â &#8617; . |",
            "url": "https://jinmang2.github.io/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " â€¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! Iâ€™m MyunghoonJin . This website 1 (powered by fastpages) is the storage of my trial and error reports from experience or taking lectures. So it maybe contains inaccurate contents. If you find something, or have a question about contents, just feel free to contact me through mail or comment it in each page (comments are linked to my github with utteranc.es) . NothingÂ &#8617; . |",
          "url": "https://jinmang2.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://jinmang2.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}