---
toc: true
layout: post
description: NIPS 2017 논문 리뷰
categories: [Fundamental]
title: [Paper] The Reversible Residual Networks: Backpropagation Without Storing Activations
image: images/revnet_eq8.PNG
---

# RevNet
- 2017년도에 나온 논문
- 딥러닝 논문 읽기 모임에서 h-transformer-1d 논문을 뜯어보다 발견한 기법
- 찾아보니 lucidrains님이 reformer를 구현하며 사용한 `_ReversibleFunction`을 이후 transformer 구현체에 사용하는 것을 발견
- residual connection의 단점인 Memory consumption을 개선한 기법

$y1=x1+F(x2)$

$$
\begin{aligned}\\
y_1=x_1+F(x_2)\\
y_2=x_2+G(y_1)
\end{aligned}\\
$$
